<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Sidekick Embed</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'brand-teal': '#01a4a6',
            'brand-orange': '#fc7244',
            'dark-bg': '#000000',
            'dark-surface': 'rgb(20, 20, 20)',
            'dark-elevated': '#252525',
            'dark-border': '#2a2a2a',
            'dark-text': '#e0e0e0',
            'dark-text-secondary': '#a0a0a0'
          },
          ringColor: {
            DEFAULT: '#01a4a6'
          }
        }
      }
    }
  </script>
  <style>
    html, body { height: 100%; }
    html { background: #000000 !important; }
    body { margin: 0; background: #000000 !important; color:#e0e0e0; height:100%; }
    .htmx-indicator { opacity: 0; transition: opacity 300ms ease; }
    .htmx-request .htmx-indicator { opacity: 1; }
    /* Button inline loading */
    .btn-loading { position: relative; opacity: 0.9; pointer-events: none; }
    .btn-loading > * { visibility: hidden; }
    .btn-loading::after { content:""; position:absolute; left:50%; top:50%; transform:translate(-50%,-50%); width:16px; height:16px; border:2px solid #01a4a6; border-top-color:#fc7244; border-radius:9999px; animation: spin 0.9s linear infinite; }
    @keyframes spin { 0%{transform:translate(-50%,-50%) rotate(0deg);} 100%{transform:translate(-50%,-50%) rotate(360deg);} }

    /* Initial loading overlay */
    #initialLoader {
      position: absolute;
      inset: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      background: rgba(0, 0, 0, 0.95);
      backdrop-filter: blur(8px);
      -webkit-backdrop-filter: blur(8px);
      z-index: 100;
      transition: opacity 0.3s ease, visibility 0.3s ease;
    }
    #initialLoader.fade-out {
      opacity: 0;
      visibility: hidden;
    }
    /* Modern animated loader - orbital rings */
    .orbital-loader {
      position: relative;
      width: 60px;
      height: 60px;
    }
    .orbital-loader .ring {
      position: absolute;
      inset: 0;
      border-radius: 50%;
      border-width: 3px;
      border-style: solid;
      border-color: #01a4a6;
      box-sizing: border-box;
      --tw-ring-color: transparent;
      --tw-ring-offset-color: transparent;
      box-shadow: none;
      outline: none;
    }
    .orbital-loader .ring:nth-child(1) {
      border-top-color: #fc7244;
      animation: orbit 1.2s cubic-bezier(0.5, 0, 0.5, 1) infinite;
    }
    .orbital-loader .ring:nth-child(2) {
      inset: 8px;
      border-right-color: #fc7244;
      animation: orbit 1.2s cubic-bezier(0.5, 0, 0.5, 1) infinite;
      animation-delay: -0.4s;
    }
    .orbital-loader .ring:nth-child(3) {
      inset: 16px;
      border-bottom-color: #fc7244;
      animation: orbit 1.2s cubic-bezier(0.5, 0, 0.5, 1) infinite;
      animation-delay: -0.8s;
    }
    @keyframes orbit {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }
    .loader-text {
      margin-top: 16px;
      color: rgba(255, 255, 255, 0.6);
      font-size: 13px;
      font-weight: 500;
      letter-spacing: 0.5px;
    }
    .loader-dot {
      display: inline-block;
      animation: dotPulse 1.4s infinite;
    }
    .loader-dot:nth-child(2) { animation-delay: 0.2s; }
    .loader-dot:nth-child(3) { animation-delay: 0.4s; }
    @keyframes dotPulse {
      0%, 80%, 100% { opacity: 0.3; }
      40% { opacity: 1; }
    }

    /* Ambient ability notification toast - top right like save confirmations */
    .ambient-notification {
      position: absolute;
      top: 16px;
      right: 16px;
      transform: translateX(20px);
      background: linear-gradient(135deg, rgba(1, 164, 166, 0.25), rgba(252, 114, 68, 0.15));
      border: 1px solid rgba(1, 164, 166, 0.5);
      border-radius: 12px;
      padding: 12px 16px;
      display: flex;
      align-items: center;
      gap: 10px;
      opacity: 0;
      visibility: hidden;
      transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
      z-index: 60;
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
      max-width: 280px;
    }
    .ambient-notification.show {
      opacity: 1;
      visibility: visible;
      transform: translateX(0);
    }
    .ambient-notification .notification-icon {
      font-size: 16px;
      animation: sparkle 2s ease-in-out infinite;
    }
    @keyframes sparkle {
      0%, 100% { opacity: 1; transform: scale(1); }
      50% { opacity: 0.7; transform: scale(1.1); }
    }
    .ambient-notification .notification-text {
      color: rgba(255, 255, 255, 0.95);
      font-size: 13px;
      font-weight: 500;
      letter-spacing: 0.3px;
    }

    /* Force black background on everything */
    body, html {
      background: #000000 !important;
      background-color: #000000 !important;
    }
    /* Override prose paragraph spacing for better readability in voice transcripts */
    .prose p {
      margin-top: 1em;
      margin-bottom: 1em;
    }
    .prose p:first-child {
      margin-top: 0;
    }
    .prose p:last-child {
      margin-bottom: 0;
    }

    /* Dark scrollbar styles */
    ::-webkit-scrollbar {
      width: 8px;
      height: 8px;
    }
    ::-webkit-scrollbar-track {
      background: rgba(255, 255, 255, 0.05);
      border-radius: 4px;
    }
    ::-webkit-scrollbar-thumb {
      background: rgba(255, 255, 255, 0.15);
      border-radius: 4px;
    }
    ::-webkit-scrollbar-thumb:hover {
      background: rgba(255, 255, 255, 0.25);
    }
    /* Firefox scrollbar */
    * {
      scrollbar-width: thin;
      scrollbar-color: rgba(255, 255, 255, 0.15) rgba(255, 255, 255, 0.05);
    }

    /* Glass morphism styles */
    .glass-container {
      background: rgba(15, 15, 15, 0.85);
      backdrop-filter: blur(20px);
      -webkit-backdrop-filter: blur(20px);
      border: 1px solid rgba(255, 255, 255, 0.08);
    }
    .glass-surface {
      background: rgba(255, 255, 255, 0.03);
      backdrop-filter: blur(10px);
      -webkit-backdrop-filter: blur(10px);
      border: 1px solid rgba(255, 255, 255, 0.06);
    }
    .glass-input {
      background: rgba(255, 255, 255, 0.05);
      backdrop-filter: blur(10px);
      -webkit-backdrop-filter: blur(10px);
      border: 1px solid rgba(255, 255, 255, 0.1);
      transition: all 0.2s ease;
    }
    .glass-input:focus {
      background: rgba(255, 255, 255, 0.08);
      border-color: rgba(1, 164, 166, 0.5);
      box-shadow: 0 0 20px rgba(1, 164, 166, 0.15);
      outline: none;
    }
    .glass-input::placeholder {
      color: rgba(255, 255, 255, 0.4);
    }

    /* Mode toggle buttons */
    .mode-toggle {
      display: inline-flex;
      padding: 3px;
      background: rgba(255, 255, 255, 0.05);
      border-radius: 8px;
      border: 1px solid rgba(255, 255, 255, 0.08);
    }
    .mode-toggle-btn {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 500;
      border-radius: 6px;
      transition: all 0.2s ease;
      color: rgba(255, 255, 255, 0.5);
      background: transparent;
      border: none;
      cursor: pointer;
    }
    .mode-toggle-btn:hover {
      color: rgba(255, 255, 255, 0.8);
      background: rgba(255, 255, 255, 0.05);
    }
    .mode-toggle-btn.active {
      color: white;
      background: rgba(1, 164, 166, 0.9);
      box-shadow: 0 2px 8px rgba(1, 164, 166, 0.3);
    }
    .mode-toggle-btn svg {
      width: 16px;
      height: 16px;
    }
    /* Disabled mode styling */
    .mode-toggle-btn.disabled-mode {
      opacity: 0.35;
      cursor: not-allowed;
      position: relative;
    }
    .mode-toggle-btn.disabled-mode:hover {
      color: rgba(255, 255, 255, 0.5);
      background: transparent;
    }
    .mode-toggle-btn.disabled-mode::after {
      content: '';
      position: absolute;
      top: 50%;
      left: 8px;
      right: 8px;
      height: 1px;
      background: rgba(255, 255, 255, 0.4);
    }

    /* Agent profile display in header */
    .agent-profile {
      display: flex;
      align-items: center;
      gap: 10px;
    }
    .agent-profile-image {
      width: 32px;
      height: 32px;
      border-radius: 50%;
      object-fit: cover;
      border: 2px solid rgba(1, 164, 166, 0.5);
      background: rgba(255, 255, 255, 0.1);
    }
    .agent-profile-placeholder {
      width: 32px;
      height: 32px;
      border-radius: 50%;
      background: linear-gradient(135deg, rgba(1, 164, 166, 0.8), rgba(1, 164, 166, 0.4));
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 14px;
      font-weight: 600;
      color: white;
      border: 2px solid rgba(1, 164, 166, 0.5);
    }
    .agent-profile-name {
      font-size: 14px;
      font-weight: 500;
      color: rgba(255, 255, 255, 0.85);
      max-width: 140px;
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
    }

    /* Disabled mode tooltip */
    .disabled-mode-tooltip {
      position: fixed;
      bottom: 80px;
      left: 50%;
      transform: translateX(-50%);
      background: rgba(0, 0, 0, 0.9);
      color: white;
      padding: 10px 16px;
      border-radius: 8px;
      font-size: 13px;
      z-index: 1000;
      border: 1px solid rgba(255, 255, 255, 0.1);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    }
    .animate-fade-in {
      animation: fadeInOut 2.5s ease-in-out;
    }
    @keyframes fadeInOut {
      0% { opacity: 0; transform: translateX(-50%) translateY(10px); }
      15% { opacity: 1; transform: translateX(-50%) translateY(0); }
      85% { opacity: 1; transform: translateX(-50%) translateY(0); }
      100% { opacity: 0; transform: translateX(-50%) translateY(-10px); }
    }

    /* Message bubbles */
    .msg-user {
      background: linear-gradient(135deg, rgba(1, 164, 166, 0.2) 0%, rgba(1, 164, 166, 0.1) 100%);
      border: 1px solid rgba(1, 164, 166, 0.2);
    }
    .msg-assistant {
      background: rgba(255, 255, 255, 0.04);
      border: 1px solid rgba(255, 255, 255, 0.08);
    }

    /* Video transcript bubbles - compact, less obtrusive */
    .video-transcript-user {
      background: linear-gradient(135deg, rgba(1, 164, 166, 0.35) 0%, rgba(1, 164, 166, 0.2) 100%);
      border: 1px solid rgba(1, 164, 166, 0.3);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
    }
    .video-transcript-assistant {
      background: rgba(0, 0, 0, 0.5);
      border: 1px solid rgba(255, 255, 255, 0.1);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
    }
    /* Fade-in animation for new transcripts */
    .video-transcript-bubble {
      animation: transcriptFadeIn 0.3s ease-out;
    }
    @keyframes transcriptFadeIn {
      from { opacity: 0; transform: translateY(8px); }
      to { opacity: 1; transform: translateY(0); }
    }
    /* Fade out old transcripts */
    .video-transcript-fade {
      opacity: 0.5;
      transition: opacity 0.5s ease;
    }

    /* Send button */
    .send-btn {
      background: linear-gradient(135deg, #01a4a6 0%, #018a8c 100%);
      border: none;
      padding: 10px 16px;
      border-radius: 10px;
      color: white;
      font-weight: 500;
      font-size: 14px;
      cursor: pointer;
      transition: all 0.2s ease;
      display: inline-flex;
      align-items: center;
      gap: 6px;
    }
    .send-btn:hover {
      transform: translateY(-1px);
      box-shadow: 0 4px 12px rgba(1, 164, 166, 0.4);
    }
    .send-btn:active {
      transform: translateY(0);
    }

    /* History button */
    .history-btn {
      background: transparent;
      border: 1px solid rgba(255, 255, 255, 0.1);
      color: rgba(255, 255, 255, 0.5);
      transition: all 0.2s ease;
    }
    .history-btn:hover {
      background: rgba(255, 255, 255, 0.05);
      border-color: rgba(255, 255, 255, 0.15);
      color: rgba(255, 255, 255, 0.8);
    }

    /* Subtle glow effect for active elements */
    .glow-teal {
      box-shadow: 0 0 30px rgba(1, 164, 166, 0.1);
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/@supabase/supabase-js@2.47.10/dist/umd/supabase.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/dompurify@3.1.6/dist/purify.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/livekit-client@2/dist/livekit-client.umd.min.js"></script>
  {% if supertab_config and supertab_config.enabled %}
  <script type="module">
    // Load Supertab SDK via ES module import
    (async function() {
      try {
        const { Supertab } = await import('https://js.supertab.co/v3/supertab.js');
        window.Supertab = Supertab;
        window.supertabReady = true;
        window.dispatchEvent(new Event('supertab-ready'));
        console.log('[supertab] SDK loaded via ES module import');
      } catch (err) {
        console.error('[supertab] Failed to load SDK:', err);
        window.supertabReady = false;
        window.supertabLoadError = err;
      }
    })();
  </script>
  {% endif %}
  <link rel="stylesheet" href="/static/css/citations.css?v=20260114b">
  <link rel="stylesheet" href="/static/css/widgets.css?v=20260114e">
  <script src="/static/js/citations.js?v=20260114c"></script>
  <script src="/static/js/widgets.js?v=20260102c"></script>
  <script src="/static/js/content-catalyst-widget.js?v=20260114c"></script>
</head>
<body style="background: #000000;">
  <div class="w-full p-3 md:p-4 h-full" style="min-height: min(700px, 95vh);">
    <div class="glass-container rounded-2xl h-full flex flex-col overflow-hidden relative" style="min-height: min(650px, 90vh);">
      <!-- Initial loading overlay - shown while checking auth -->
      <div id="initialLoader">
        <div class="orbital-loader">
          <div class="ring"></div>
          <div class="ring"></div>
          <div class="ring"></div>
        </div>
        <div class="loader-text">
          Loading<span class="loader-dot">.</span><span class="loader-dot">.</span><span class="loader-dot">.</span>
        </div>
      </div>

      <!-- Ambient ability notification toast -->
      <div id="ambientNotification" class="ambient-notification">
        <span class="notification-icon">âœ¨</span>
        <span class="notification-text">User Understanding Expanded</span>
      </div>

      <div id="auth" class="p-5 space-y-3 hidden">
        <h3 class="text-lg font-medium text-white">Sign in</h3>
        <input id="email" class="w-full glass-input text-white rounded-lg px-4 py-3" placeholder="Email address" />
        <input id="password" class="w-full glass-input text-white rounded-lg px-4 py-3" placeholder="Password" type="password" />
        <div class="flex gap-3 pt-1">
          <button class="send-btn flex-1" onclick="login()">Sign In</button>
          <button class="flex-1 py-2 px-4 rounded-lg text-sm font-medium history-btn" onclick="signup()">Create Account</button>
        </div>
        <div id="auth-msg" class="text-dark-text-secondary text-sm"></div>
      </div>
      <div id="chat" class="hidden flex-1 flex flex-col min-h-0">
        <!-- Header with mode toggle and new conversation button - all in one row -->
        <div class="px-4 py-3 flex items-center justify-between relative border-b border-white/5">
          <!-- Mode toggle - left side -->
          <div class="mode-toggle">
            <button id="tab-text" class="mode-toggle-btn active" onclick="switchTab('text')">
              <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z"/></svg>
              <span>Text</span>
            </button>
            <button id="tab-voice" class="mode-toggle-btn" onclick="switchTab('voice')">
              <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"/></svg>
              <span>Voice</span>
            </button>
            <button id="tab-video" class="mode-toggle-btn" onclick="switchTab('video')">
              <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 10l4.553-2.276A1 1 0 0121 8.618v6.764a1 1 0 01-1.447.894L15 14M5 18h8a2 2 0 002-2V8a2 2 0 00-2-2H5a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
              <span>Video</span>
            </button>
          </div>
          <!-- New Conversation button - absolutely centered -->
          <button id="newConversationBtn" onclick="startNewConversation()" class="absolute left-1/2 -translate-x-1/2 inline-flex items-center gap-2 px-3 py-1.5 text-sm font-medium text-white/70 hover:text-white bg-white/5 hover:bg-white/10 rounded-lg transition-all border border-white/10 hover:border-white/20">
            <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 4v16m8-8H4"/></svg>
            <span>New Conversation</span>
          </button>
          <!-- Right side: Agent profile + End Call button -->
          <div class="flex items-center gap-3">
            <!-- Agent profile - right aligned -->
            <div class="agent-profile" id="agentProfileHeader">
              <span class="agent-profile-name">{{ agent_name }}</span>
              {% if agent_image %}
              <img src="{{ agent_image }}" alt="{{ agent_name }}" class="agent-profile-image" onerror="this.style.display='none'; this.nextElementSibling.style.display='flex';">
              <div class="agent-profile-placeholder" style="display: none;">{{ agent_name[0] | upper if agent_name else 'S' }}</div>
              {% else %}
              <div class="agent-profile-placeholder">{{ agent_name[0] | upper if agent_name else 'S' }}</div>
              {% endif %}
            </div>
            <!-- End Call button (shown during voice call) -->
            <button id="voiceEndBtn" class="hidden inline-flex items-center gap-2 px-3 py-1.5 bg-red-600 text-white text-sm rounded-lg hover:bg-red-500 transition-all" onclick="withBtnLoading(this, endVoice)">
              <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg>
              <span>End</span>
            </button>
          </div>
        </div>
        <div id="panel-text" class="flex-1 flex flex-col min-h-0">
          <div class="flex-1 flex flex-col min-h-0">
            <div id="chatMessages" class="flex-1 overflow-y-auto p-4 space-y-4 min-h-0">
              <div id="welcomeMessage" class="text-center text-white/40 text-sm py-12">
                <svg class="w-12 h-12 mx-auto mb-3 text-white/20" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z"/></svg>
                <p>Start a conversation with your sidekick</p>
              </div>
            </div>
            <div class="glass-surface border-t-0 border-x-0 rounded-none p-4">
              <form onsubmit="return sendMsg(event)" class="flex gap-3">
                <input id="msg" name="message" placeholder="Type a message..." class="flex-1 glass-input text-white rounded-xl px-4 py-3" autocomplete="off" required />
                <button type="submit" class="send-btn">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 19l9 2-9-18-9 18 9-2zm0 0v-8"/></svg>
                  <span>Send</span>
                </button>
              </form>
              <button id="historyBtn" onclick="openConversationHistory()" class="w-full mt-3 py-2.5 px-4 text-sm history-btn rounded-xl flex items-center justify-center gap-2">
                <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"/></svg>
                <span>View Conversation History</span>
              </button>
            </div>
          </div>
        </div>
        <div id="panel-voice" class="hidden flex-1 flex flex-col min-h-0">
          <div class="flex-1 flex flex-col min-h-0 p-6 items-center justify-center">
            <!-- Supertab Paywall Section (shown when Supertab is enabled and user has no entitlement) -->
            {% if supertab_config and supertab_config.enabled %}
            <div id="supertabPaywallSection" class="text-center max-w-sm mx-auto">
              <!-- Price Badge -->
              <div class="mb-4">
                <span class="inline-flex items-center px-4 py-2 rounded-full bg-gradient-to-r from-emerald-500/20 to-teal-500/20 border border-emerald-500/30">
                  <span class="text-2xl font-bold text-white" id="supertabPriceDisplay">{{ supertab_config.price if supertab_config else 'per session' }}</span>
                </span>
              </div>

              <!-- Call to Action -->
              <p class="text-white/80 text-base mb-6" id="supertabCtaDisplay">{{ supertab_config.cta if supertab_config else 'Start a voice conversation' }}</p>

              <!-- Supertab Purchase Button Container -->
              <div id="supertabButtonContainer" class="flex justify-center mb-4">
                <!-- Supertab SDK will inject the purchase button here -->
                <div id="supertabButtonLoading" class="flex items-center gap-2 text-white/50">
                  <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:0ms"></div>
                  <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:150ms"></div>
                  <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:300ms"></div>
                </div>
              </div>

              <!-- Disclaimer Notes -->
              <div class="text-xs text-white/40 space-y-1 mt-6 border-t border-white/10 pt-4">
                <p class="font-medium text-white/50">Please Note</p>
                <p>Single-use, tied to your device</p>
                <p>Reloading restarts the session</p>
                <p>Pricing may change</p>
              </div>
            </div>
            {% endif %}

            <!-- Regular Voice Welcome (shown when no Supertab or user has entitlement) -->
            <div id="voiceWelcome" class="text-center {% if supertab_config and supertab_config.enabled %}hidden{% endif %}">
              <div class="text-white/40 mb-6 text-sm">Start a voice chat with your sidekick</div>
              <button id="voiceStartBtn" class="send-btn px-6 py-3" onclick="withBtnLoading(this, startVoice)">
                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"/></svg>
                <span>Start Voice Chat</span>
              </button>
            </div>

            <!-- Voice Session Active Area -->
            <div id="voiceTranscriptList" class="w-full flex-1 overflow-y-auto p-4 space-y-4 min-h-0 hidden"></div>
            <div id="agentSpeakingIndicator" class="hidden mb-2">
              <div class="flex items-center gap-2 text-dark-text-secondary text-sm">
                <span>Agent speaking</span>
                <div class="flex space-x-1">
                  <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:0ms"></div>
                  <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:150ms"></div>
                  <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:300ms"></div>
                </div>
              </div>
            </div>
            <div id="voiceStatus" class="text-xs text-dark-text-secondary text-center"></div>
          </div>
          <!-- Voice History Button -->
          <div class="glass-surface border-t-0 border-x-0 rounded-none p-4">
            <button onclick="openConversationHistory()" class="w-full py-2.5 px-4 text-sm history-btn rounded-xl flex items-center justify-center gap-2">
              <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"/></svg>
              <span>View Conversation History</span>
            </button>
          </div>
        </div>
        <!-- Video Chat Panel -->
        <div id="panel-video" class="hidden flex-1 flex flex-col min-h-0">
          <div class="flex-1 flex flex-col min-h-0 p-4 items-center justify-center relative">
            <!-- Video Welcome (shown before video starts) -->
            <div id="videoWelcome" class="text-center">
              <div class="text-white/40 mb-6 text-sm">Start a video chat with your sidekick's avatar</div>
              <button id="videoStartBtn" class="send-btn px-6 py-3" onclick="withBtnLoading(this, startVideo)">
                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 10l4.553-2.276A1 1 0 0121 8.618v6.764a1 1 0 01-1.447.894L15 14M5 18h8a2 2 0 002-2V8a2 2 0 00-2-2H5a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
                <span>Start Video Chat</span>
              </button>
            </div>
            
            <!-- Video Container (hidden until video starts) -->
            <div id="videoContainer" class="hidden absolute inset-0 bg-black flex items-center justify-center">
              <video id="remoteVideoElement" autoplay playsinline class="w-full h-full object-contain"></video>

              <!-- Model Loading Overlay (shown while avatar model loads) -->
              <div id="videoModelLoader" class="absolute inset-0 bg-black flex flex-col items-center justify-center z-10">
                <div class="orbital-loader mb-6">
                  <div class="ring"></div>
                  <div class="ring"></div>
                  <div class="ring"></div>
                </div>
                <div id="videoLoaderText" class="text-white/70 text-sm font-medium mb-4">Initializing avatar...</div>
                <div class="w-48 h-1.5 bg-white/10 rounded-full overflow-hidden">
                  <div id="videoLoaderProgress" class="h-full bg-gradient-to-r from-brand-teal to-brand-orange rounded-full transition-all duration-500 ease-out" style="width: 5%"></div>
                </div>
                <div id="videoLoaderPercent" class="text-white/40 text-xs mt-2">5%</div>
              </div>

              <!-- Transcript overlay at bottom -->
              <div id="videoTranscriptOverlay" class="absolute bottom-0 left-0 right-0 p-3 pointer-events-none">
                <div id="videoTranscriptList" class="max-h-28 overflow-hidden space-y-1.5 flex flex-col justify-end"></div>
              </div>
              <!-- End Video button - top right -->
              <button id="videoEndBtn" class="absolute top-4 right-4 inline-flex items-center gap-2 px-3 py-1.5 bg-red-600 text-white text-sm rounded-lg hover:bg-red-500 transition-all" onclick="withBtnLoading(this, endVideo)">
                <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg>
                <span>End</span>
              </button>
              <!-- Fullscreen toggle - top left -->
              <button id="videoFullscreenBtn" class="absolute top-4 left-4 p-2 bg-white/10 hover:bg-white/20 text-white rounded-lg transition-all" onclick="toggleVideoFullscreen()">
                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 8V4m0 0h4M4 4l5 5m11-5h-4m4 0v4m0-4l-5 5M4 16v4m0 0h4m-4 0l5-5m11 5h-4m4 0v-4m0 4l-5-5"/></svg>
              </button>
            </div>
            
            <div id="videoStatus" class="text-xs text-dark-text-secondary text-center mt-4"></div>
          </div>
          <!-- Video History Button -->
          <div class="glass-surface border-t-0 border-x-0 rounded-none p-4">
            <button onclick="openConversationHistory()" class="w-full py-2.5 px-4 text-sm history-btn rounded-xl flex items-center justify-center gap-2">
              <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"/></svg>
              <span>View Conversation History</span>
            </button>
          </div>
        </div>
      </div>
    </div>

    <!-- Conversation History Overlay -->
    <div id="historyOverlay" class="hidden fixed inset-0 bg-black/75 z-50 flex items-end justify-center" style="backdrop-filter: blur(4px); -webkit-backdrop-filter: blur(4px);">
      <div class="glass-container rounded-t-2xl w-full max-w-[900px] max-h-[85vh] flex flex-col animate-slide-up">
        <div class="p-4 border-b border-white/5 flex items-center justify-between">
          <h3 class="text-lg font-medium text-white">Conversation History</h3>
          <button onclick="closeConversationHistory()" class="text-white/50 hover:text-white p-1 transition-colors">
            <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg>
          </button>
        </div>
        <div class="p-4 border-b border-white/5">
          <button onclick="startNewConversation()" class="send-btn w-full py-3 justify-center">
            <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 4v16m8-8H4"/></svg>
            <span>Start New Conversation</span>
          </button>
        </div>
        <div id="historyList" class="flex-1 overflow-y-auto p-4 space-y-2">
          <div id="historyLoading" class="text-center text-white/40 py-8">
            <div class="flex justify-center space-x-2 mb-2">
              <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:0ms"></div>
              <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:150ms"></div>
              <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:300ms"></div>
            </div>
            <p>Loading conversations...</p>
          </div>
          <div id="historyEmpty" class="hidden text-center text-white/40 py-8">
            <p>No previous conversations found.</p>
            <p class="text-xs mt-2">Start a new conversation to begin!</p>
          </div>
        </div>
      </div>
    </div>

    <!-- Supertab Purchase Overlay - DEPRECATED: Now using inline paywall in voice panel -->
  </div>

  <style>
    @keyframes slide-up {
      from { transform: translateY(100%); opacity: 0; }
      to { transform: translateY(0); opacity: 1; }
    }
    .animate-slide-up {
      animation: slide-up 0.3s ease-out;
    }
  </style>

  <script>
    try { console.log('[voice] embed script initializing'); } catch {}
    try {
      window.addEventListener('error', (event) => {
        try {
          const { message, filename, lineno, colno } = event || {};
          console.error('[voice] window error', message, filename, lineno, colno, event?.error);
        } catch {}
      });
      window.addEventListener('unhandledrejection', (event) => {
        try { console.error('[voice] unhandled rejection', event?.reason); } catch {}
      });
    } catch {}

    function setBtnLoading(btn, isLoading){ try { if(!btn) return; btn.classList.toggle('btn-loading', !!isLoading); btn.disabled = !!isLoading; } catch{} }
    async function withBtnLoading(btn, fn){ setBtnLoading(btn, true); try { return await fn(); } finally { setBtnLoading(btn, false); } }
    const CLIENT_ID = "{{ client_id }}";
    const AGENT_ID = "{{ agent_id or '' }}";
    const AGENT_SLUG = "{{ agent_slug }}";
    const SUPABASE_URL = "{{ supabase_url }}";
    const SUPABASE_ANON = "{{ supabase_anon_key }}";
    const sb = supabase.createClient(SUPABASE_URL, SUPABASE_ANON);
    const CLIENT_SUPABASE_URL = "{{ client_supabase_url }}";
    const CLIENT_SUPABASE_ANON = "{{ client_supabase_anon_key }}";
    const CLIENT_SUPABASE_BOOTSTRAP = {
      access_token: "{{ client_supabase_access_token | default('') }}",
      refresh_token: "{{ client_supabase_refresh_token | default('') }}"
    };
    const LIVEKIT_TRANSCRIPT_FALLBACK = Boolean(window.__enableLivekitTranscriptFallback) || (!CLIENT_SUPABASE_URL || !CLIENT_SUPABASE_ANON);
    let transcriptSupabase = null;
    let transcriptRealtimeChannel = null;
    let transcriptConversationId = null;
    let session = null;
    let manualToken = null; // token provided by parent preview via postMessage
    window.__lk_audio_elements = window.__lk_audio_elements || {};
    window.__lk_transcript_seen = window.__lk_transcript_seen || {};

    // Supertab configuration
    const SUPERTAB_CONFIG = {
      enabled: {{ 'true' if supertab_config and supertab_config.enabled else 'false' }},
      clientId: "{{ supertab_config.client_id if supertab_config else '' }}",
      experienceId: "{{ supertab_config.experience_id if supertab_config else '' }}",
      price: "{{ supertab_config.price if supertab_config else 'per session' }}",
      cta: "{{ supertab_config.cta if supertab_config else 'Start a voice conversation' }}"
    };

    // Chat mode configuration
    const VOICE_CHAT_ENABLED = {{ 'true' if voice_chat_enabled else 'false' }};
    const VIDEO_CHAT_ENABLED = {{ 'true' if video_chat_enabled else 'false' }};
    const TEXT_CHAT_ENABLED = {{ 'true' if text_chat_enabled else 'false' }};

    let supertabClient = null;
    let supertabPurchaseButton = null;
    let hasVoiceEntitlement = false;
    let supertabResolve = null;  // Promise resolver for purchase flow

    // Conversation history state
    let currentConversationId = null;
    let isFirstMessage = true;

    // Get effective user ID for conversation history
    // For admin preview, __client_user_id takes precedence (set via postMessage from parent)
    // This ensures the correct shadow user ID is used rather than the session's user ID
    function getEffectiveUserId() {
      // Admin preview mode: use the explicitly provided client_user_id
      if (window.__client_user_id) return window.__client_user_id;
      // Normal mode: use session user ID
      if (session && session.user && session.user.id) return session.user.id;
      return null;
    }

    // Open conversation history overlay
    async function openConversationHistory() {
      const overlay = document.getElementById('historyOverlay');
      const loading = document.getElementById('historyLoading');
      const empty = document.getElementById('historyEmpty');
      const list = document.getElementById('historyList');

      overlay.classList.remove('hidden');
      loading.classList.remove('hidden');
      empty.classList.add('hidden');

      // Clear previous items (keep loading and empty divs)
      Array.from(list.children).forEach(child => {
        if (child.id !== 'historyLoading' && child.id !== 'historyEmpty') {
          child.remove();
        }
      });

      const userId = getEffectiveUserId();
      if (!userId) {
        loading.classList.add('hidden');
        empty.classList.remove('hidden');
        empty.innerHTML = '<p>Please sign in to view conversation history.</p>';
        return;
      }

      try {
        const params = new URLSearchParams({
          client_id: CLIENT_ID,
          user_id: userId,
          agent_slug: AGENT_SLUG,
          limit: '50'
        });

        const res = await fetch(`/api/embed/conversations?${params}`);
        const data = await res.json();

        loading.classList.add('hidden');

        if (!data.success || !data.conversations || data.conversations.length === 0) {
          empty.classList.remove('hidden');
          return;
        }

        // Render conversation items
        data.conversations.forEach(conv => {
          const item = document.createElement('div');
          item.className = 'p-3 glass-surface rounded-xl hover:border-brand-teal/50 cursor-pointer transition-all';
          item.onclick = () => resumeConversation(conv.id);

          const title = conv.conversation_title || 'Untitled Conversation';
          const date = conv.updated_at || conv.created_at;
          const formattedDate = date ? new Date(date).toLocaleDateString('en-US', { month: 'short', day: 'numeric', hour: '2-digit', minute: '2-digit' }) : '';
          const preview = conv.last_message ? conv.last_message.content : 'No messages yet';
          const isActive = conv.id === currentConversationId;

          item.innerHTML = `
            <div class="flex items-start justify-between gap-2">
              <div class="flex-1 min-w-0">
                <div class="flex items-center gap-2">
                  <h4 class="text-white font-medium truncate">${escapeHtml(title)}</h4>
                  ${isActive ? '<span class="text-xs bg-brand-teal text-white px-2 py-0.5 rounded-md">Current</span>' : ''}
                </div>
                <p class="text-white/50 text-sm truncate mt-1">${escapeHtml(preview)}</p>
              </div>
              <span class="text-white/40 text-xs whitespace-nowrap">${formattedDate}</span>
            </div>
          `;

          list.appendChild(item);
        });

      } catch (err) {
        console.error('[history] Failed to load conversations:', err);
        loading.classList.add('hidden');
        empty.classList.remove('hidden');
        empty.innerHTML = '<p>Failed to load conversations.</p>';
      }
    }

    // Close conversation history overlay
    function closeConversationHistory() {
      document.getElementById('historyOverlay').classList.add('hidden');
    }

    // Start a new conversation
    function startNewConversation() {
      currentConversationId = null;
      isFirstMessage = true;
      clearChatMessages();
      closeConversationHistory();
      console.log('[history] Started new conversation');
    }

    // ========================================
    // Supertab Paywall Functions
    // ========================================

    async function waitForSupertabSDK(timeout = 5000) {
      // If already ready, return immediately
      if (window.supertabReady && window.Supertab) {
        return true;
      }

      // Wait for the supertab-ready event
      return new Promise((resolve) => {
        const timeoutId = setTimeout(() => {
          console.warn('[supertab] SDK load timeout');
          resolve(false);
        }, timeout);

        window.addEventListener('supertab-ready', () => {
          clearTimeout(timeoutId);
          resolve(true);
        }, { once: true });
      });
    }

    async function initSupertab() {
      if (!SUPERTAB_CONFIG.enabled || !SUPERTAB_CONFIG.clientId) {
        console.log('[supertab] Not enabled or missing config');
        return;
      }

      // Hide loading indicator
      const loadingEl = document.getElementById('supertabButtonLoading');

      try {
        // Wait for SDK to load
        const sdkReady = await waitForSupertabSDK();
        if (!sdkReady || typeof window.Supertab === 'undefined') {
          console.warn('[supertab] SDK not loaded after waiting');
          if (loadingEl) loadingEl.innerHTML = '<span class="text-red-400">Failed to load payment options</span>';
          return;
        }

        supertabClient = new window.Supertab({ clientId: SUPERTAB_CONFIG.clientId });
        console.log('[supertab] Initialized with clientId:', SUPERTAB_CONFIG.clientId);

        // Create purchase button for this experience (single item / per-session billing)
        if (SUPERTAB_CONFIG.experienceId) {
          const buttonContainer = document.getElementById('supertabButtonContainer');

          if (buttonContainer) {
            // Hide loading indicator once we start creating the button
            if (loadingEl) loadingEl.classList.add('hidden');

            const { initialState, show } = await supertabClient.createPurchaseButton({
              experienceId: SUPERTAB_CONFIG.experienceId,
              containerElement: buttonContainer,
              onDone: ({ priorEntitlement, purchase }) => {
                console.log('[supertab] onDone callback', { priorEntitlement, purchase });

                if (priorEntitlement) {
                  // User already has access
                  console.log('[supertab] User has prior entitlement');
                  hasVoiceEntitlement = true;
                  handleSupertabSuccess({ status: 'entitled', priorEntitlement });
                } else if (purchase && purchase.status === 'completed') {
                  // User completed purchase
                  console.log('[supertab] Purchase completed!', purchase);
                  hasVoiceEntitlement = true;
                  handleSupertabSuccess({ status: 'purchased', purchase });
                } else {
                  // User cancelled - stay on paywall
                  console.log('[supertab] Purchase cancelled or abandoned');
                }
              }
            });

            supertabPurchaseButton = { initialState, show };
            console.log('[supertab] Purchase button created, initialState:', initialState);

            // Check if user already has entitlement - show voice UI immediately
            if (initialState === 'entitled') {
              hasVoiceEntitlement = true;
              console.log('[supertab] User has existing entitlement from initialState');
              showVoiceReadyUI();
            }
          }
        }
      } catch (err) {
        console.error('[supertab] Failed to initialize:', err);
        window.supertabInitError = err;
        if (loadingEl) loadingEl.innerHTML = '<span class="text-red-400">Failed to load payment options</span>';
      }
    }

    // Show the voice-ready UI (hide paywall, show start button)
    function showVoiceReadyUI() {
      const paywallSection = document.getElementById('supertabPaywallSection');
      const voiceWelcome = document.getElementById('voiceWelcome');
      const transcriptList = document.getElementById('voiceTranscriptList');

      if (paywallSection) paywallSection.classList.add('hidden');
      if (voiceWelcome) voiceWelcome.classList.remove('hidden');
      if (transcriptList) transcriptList.classList.remove('hidden');
    }

    // Handle successful Supertab purchase or entitlement
    async function handleSupertabSuccess(result) {
      console.log('[supertab] handleSupertabSuccess:', result);

      // Hide paywall section and show voice UI
      showVoiceReadyUI();

      // Create user account if we have purchase info
      if (result.purchase) {
        try {
          await handleSupertabPaymentSuccess(result);
        } catch (err) {
          console.warn('[supertab] Failed to create user after purchase:', err);
        }
      }

      // Automatically start voice chat after successful purchase
      setTimeout(() => {
        const voiceStartBtn = document.getElementById('voiceStartBtn');
        if (voiceStartBtn) {
          console.log('[supertab] Auto-starting voice chat after purchase');
          withBtnLoading(voiceStartBtn, startVoice);
        }
      }, 500);
    }

    async function checkVoiceEntitlement() {
      if (!SUPERTAB_CONFIG.enabled) return true;
      if (hasVoiceEntitlement) return true;

      // Check if purchase button reports entitlement
      if (supertabPurchaseButton) {
        try {
          if (supertabPurchaseButton.initialState === 'entitled') {
            hasVoiceEntitlement = true;
            return true;
          }
        } catch (err) {
          console.error('[supertab] Entitlement check failed:', err);
          // Fail open
          return true;
        }
      }

      return false;
    }

    // DEPRECATED: showSupertabPaywall and closeSupertabOverlay - now using inline paywall in voice panel

    // Handle successful Supertab payment - create/link user in client's Supabase
    async function handleSupertabPaymentSuccess(paymentResult) {
      console.log('[supertab] Processing payment success, creating user account...', paymentResult);

      // Get user info from Supertab - handle both paywall and purchase button formats
      const purchase = paymentResult.purchase || paymentResult;
      const supertabUser = purchase.user || purchase.customer || paymentResult.user || paymentResult.customer || {};
      const email = supertabUser.email || purchase.email;
      const supertabUserId = supertabUser.id || purchase.customerId || paymentResult.customerId;

      if (!email) {
        console.log('[supertab] No email from payment result, skipping user creation');
        return;
      }

      try {
        // Call backend API to create/link user in client's Supabase
        const response = await fetch('/api/embed/supertab/create-user', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            client_id: CLIENT_ID,
            agent_slug: AGENT_SLUG,
            email: email,
            supertab_user_id: supertabUserId,
            payment_status: paymentResult.status,
            offering_id: SUPERTAB_CONFIG.experienceId,
          }),
        });

        if (response.ok) {
          const result = await response.json();
          console.log('[supertab] User created/linked successfully:', result);

          // If a new user was created and we got credentials, sign them in
          if (result.user_id) {
            window.__supertab_user_id = result.user_id;
            // Store for conversation history
            if (!session) {
              window.__client_user_id = result.user_id;
            }
          }
        } else {
          const error = await response.text();
          console.warn('[supertab] User creation failed:', error);
        }
      } catch (err) {
        console.error('[supertab] Error creating user:', err);
      }
    }

    // Resume a previous conversation
    async function resumeConversation(conversationId) {
      closeConversationHistory();
      clearChatMessages();

      // Show loading indicator
      const chatMessages = document.getElementById('chatMessages');
      chatMessages.innerHTML = `
        <div class="text-center text-white/40 py-8">
          <div class="flex justify-center space-x-2 mb-2">
            <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:0ms"></div>
            <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:150ms"></div>
            <div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:300ms"></div>
          </div>
          <p>Loading conversation...</p>
        </div>
      `;

      try {
        const params = new URLSearchParams({
          client_id: CLIENT_ID
        });

        const res = await fetch(`/api/embed/conversations/${conversationId}/messages?${params}`);
        const data = await res.json();

        if (!data.success) {
          throw new Error('Failed to load messages');
        }

        currentConversationId = conversationId;
        isFirstMessage = false; // Not first message since we're resuming

        chatMessages.innerHTML = '';

        // Render each message
        data.messages.forEach(msg => {
          if (msg.role === 'user') {
            addUser(msg.content, false);
          } else if (msg.role === 'assistant') {
            addAssistant(msg.content, msg.citations || [], false);
          } else if (msg.role === 'widget') {
            // Restore widget from saved state
            const widgetData = msg.metadata?.widget;
            if (widgetData && widgetData.type === 'content_catalyst') {
              console.log('[history] Restoring Content Catalyst widget:', widgetData);
              addWidget('content_catalyst', {
                restoredState: widgetData
              }, false);
            }
          }
        });

        // Scroll to bottom
        scrollBottom();
        console.log('[history] Resumed conversation:', conversationId, 'with', data.messages.length, 'messages');

      } catch (err) {
        console.error('[history] Failed to load conversation:', err);
        chatMessages.innerHTML = `
          <div class="text-center text-white/40 py-8">
            <p>Failed to load conversation.</p>
            <button onclick="startNewConversation()" class="mt-2 text-brand-teal hover:underline">Start a new conversation</button>
          </div>
        `;
      }
    }

    // Clear chat messages and show welcome message
    function clearChatMessages() {
      const chatMessages = document.getElementById('chatMessages');
      chatMessages.innerHTML = `
        <div id="welcomeMessage" class="text-center text-dark-text-secondary text-sm py-8">
          <p>Start a conversation with your sidekick</p>
        </div>
      `;
    }

    // Add assistant message to chat (reusable)
    function addAssistant(text, citations = [], shouldScroll = true) {
      const chatMessages = document.getElementById('chatMessages');
      const welcome = document.getElementById('welcomeMessage');
      if (welcome) welcome.remove();

      const asstDiv = document.createElement('div');
      asstDiv.className = 'flex flex-col items-start';

      const asstBubble = document.createElement('div');
      asstBubble.className = 'max-w-[80%] px-4 py-3 rounded-2xl msg-assistant text-white/90 whitespace-pre-wrap';

      const rawText = String(text).replace(/\\n/g, '\n');
      asstBubble.innerHTML = (window.marked && window.DOMPurify) ? DOMPurify.sanitize(marked.parse(rawText)) : rawText;

      asstDiv.appendChild(asstBubble);

      // Add citations if present
      if (citations && citations.length > 0 && window.citationsComponent) {
        const messageId = `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        const citationsElement = citationsComponent.render(citations, messageId);
        if (citationsElement) {
          citationsElement.classList.add('compact');
          asstDiv.appendChild(citationsElement);
        }
      }

      chatMessages.appendChild(asstDiv);
      if (shouldScroll) scrollBottom();
    }

    // Add a widget to the chat
    function addWidget(widgetType, config = {}, shouldScroll = true) {
      const chatMessages = document.getElementById('chatMessages');
      const welcome = document.getElementById('welcomeMessage');
      if (welcome) welcome.remove();

      // Create wrapper div for the widget
      const widgetWrapper = document.createElement('div');
      widgetWrapper.className = 'flex flex-col widget-wrapper';

      // Merge config with context data
      const widgetConfig = {
        ...config,
        clientId: CLIENT_ID,
        agentId: AGENT_ID,
        agentSlug: AGENT_SLUG,
        userId: getEffectiveUserId(),
        conversationId: currentConversationId
      };

      // Create widget using registry
      if (window.widgetRegistry) {
        const widget = window.widgetRegistry.create(widgetType, widgetConfig, widgetWrapper);
        if (widget) {
          chatMessages.appendChild(widgetWrapper);
          if (shouldScroll) scrollBottom();
          console.log('[widgets] Widget added to chat:', widgetType, widget.id);
          return widget;
        }
      }

      console.error('[widgets] Failed to create widget:', widgetType);
      return null;
    }

    // Handle widget triggers from agent responses
    function handleWidgetTrigger(widgetData) {
      if (!widgetData || !widgetData.type) {
        console.warn('[widgets] Invalid widget data:', widgetData);
        return null;
      }

      console.log('[widgets] Handling widget trigger:', widgetData);
      console.log('[widgets] Widget registry available:', !!window.widgetRegistry);
      if (window.widgetRegistry) {
        console.log('[widgets] Registered widget types:', Array.from(window.widgetRegistry.widgets.keys()));
      }

      // Don't add a separate message - the agent's response already contains the text
      // The widget will appear directly after the agent's response bubble

      // Create and add the widget
      const widget = addWidget(widgetData.type, widgetData.config || {});
      if (!widget) {
        console.error('[widgets] Failed to create widget of type:', widgetData.type);
      }
      return widget;
    }

    // Generate conversation title after first message
    async function generateConversationTitle(conversationId, firstMessage) {
      try {
        const res = await fetch(`/api/embed/conversations/${conversationId}/generate-title?client_id=${CLIENT_ID}`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ first_message: firstMessage })
        });
        const data = await res.json();
        if (data.success) {
          console.log('[history] Generated title:', data.title);
        }
      } catch (err) {
        console.error('[history] Failed to generate title:', err);
      }
    }

    // Auto-restore most recent conversation on load
    async function autoRestoreConversation() {
      const userId = getEffectiveUserId();
      if (!userId) return;

      try {
        const params = new URLSearchParams({
          client_id: CLIENT_ID,
          user_id: userId,
          agent_slug: AGENT_SLUG
        });

        const res = await fetch(`/api/embed/conversations/recent?${params}`);
        const data = await res.json();

        if (data.success && data.conversation && data.messages && data.messages.length > 0) {
          currentConversationId = data.conversation.id;
          isFirstMessage = false;

          const chatMessages = document.getElementById('chatMessages');
          chatMessages.innerHTML = '';

          data.messages.forEach(msg => {
            if (msg.role === 'user') {
              addUser(msg.content, false);
            } else if (msg.role === 'assistant') {
              addAssistant(msg.content, msg.citations || [], false);
            } else if (msg.role === 'widget') {
              // Restore widget from saved state
              const widgetData = msg.metadata?.widget;
              if (widgetData && widgetData.type === 'content_catalyst') {
                console.log('[history] Restoring Content Catalyst widget:', widgetData);
                addWidget('content_catalyst', {
                  restoredState: widgetData
                }, false);
              }
            }
          });

          scrollBottom();
          console.log('[history] Auto-restored conversation:', currentConversationId);
        }
      } catch (err) {
        console.error('[history] Failed to auto-restore conversation:', err);
      }
    }

    // HTML escape helper
    function escapeHtml(text) {
      const div = document.createElement('div');
      div.textContent = text;
      return div.innerHTML;
    }

    // Convert URLs in text to clickable links (call after escapeHtml for security)
    function linkifyUrls(escapedText) {
      // URL regex pattern - matches http/https URLs
      const urlPattern = /(https?:\/\/[^\s<>"']+)/gi;
      return escapedText.replace(urlPattern, (url) => {
        // Decode HTML entities that might be in the URL from escapeHtml
        const cleanUrl = url.replace(/&amp;/g, '&');
        return `<a href="${cleanUrl}" target="_blank" rel="noopener noreferrer" class="text-brand-teal hover:underline break-all">${url}</a>`;
      });
    }

    // Escape HTML and convert URLs to clickable links
    function escapeAndLinkify(text) {
      return linkifyUrls(escapeHtml(text));
    }

    function iterateTrackPublications(participant, callback) {
      if (!participant || typeof callback !== 'function') { return; }
      const seen = new Set();
      const visit = (label, collection) => {
        if (!collection) { return; }
        const handleValue = (value, key) => {
          if (!value || seen.has(value)) { return; }
          seen.add(value);
          try { callback(value, label, key); } catch {}
        };
        if (typeof collection.forEach === 'function') {
          collection.forEach((value, key) => handleValue(value, key));
          return;
        }
        if (Array.isArray(collection)) {
          collection.forEach((value, index) => handleValue(value, index));
          return;
        }
        if (typeof collection === 'object' && typeof collection.values === 'function') {
          try {
            for (const value of collection.values()) {
              handleValue(value, undefined);
            }
          } catch {}
        }
      };
      try { visit('trackPublications', participant.trackPublications); } catch {}
      try { visit('audioTrackPublications', participant.audioTrackPublications); } catch {}
      try { visit('tracks', participant.tracks); } catch {}
      try { visit('audioTracks', participant.audioTracks); } catch {}
      try {
        if (typeof participant.getTrackPublications === 'function') {
          visit('getTrackPublications', participant.getTrackPublications());
        }
      } catch {}
      try {
        if (typeof participant.getAudioTrackPublications === 'function') {
          visit('getAudioTrackPublications', participant.getAudioTrackPublications());
        }
      } catch {}
    }

    function collectAudioPublications(participant) {
      const results = [];
      iterateTrackPublications(participant, (pub, source, key) => {
        const kind = String(pub?.kind || pub?.track?.kind || pub?.audioTrack?.kind || '').toLowerCase();
        if (kind === 'audio') {
          results.push({ pub, source, key });
          return;
        }
        const sourceLabel = String(pub?.source || '').toLowerCase();
        if (sourceLabel.includes('audio') || sourceLabel.includes('microphone') || sourceLabel.includes('speaker') || sourceLabel.includes('tts')) {
          results.push({ pub, source, key });
        }
      });
      return results;
    }

    function participantHasAudioTrack(participant) {
      let hasAudio = false;
      iterateTrackPublications(participant, (pub) => {
        if (hasAudio) { return; }
        const kind = String(pub?.kind || '').toLowerCase();
        if (kind === 'audio') { hasAudio = true; return; }
        const trackKind = String(pub?.track?.kind || pub?.audioTrack?.kind || '').toLowerCase();
        if (trackKind === 'audio') { hasAudio = true; }
      });
      return hasAudio;
    }

    function ensureAudioSubscribed(publication, contextLabel, level) {
      if (!publication) { return; }
      const label = contextLabel || 'subscribe';
      const useWarn = level === 'warn';
      const logFailure = (suffix, err) => {
        try {
          if (useWarn && console?.warn) { console.warn(`[voice] ${label} ${suffix}`, err); }
          else if (console?.error) { console.error(`[voice] ${label} ${suffix}`, err); }
        } catch {}
      };
      try {
        if (typeof publication.setSubscribed === 'function') {
          const result = publication.setSubscribed(true);
          if (result && typeof result.catch === 'function') {
            result.catch((err) => { logFailure('setSubscribed(audio) failed', err); });
          }
        } else if (typeof publication.subscribe === 'function') {
          const result = publication.subscribe();
          if (result && typeof result.catch === 'function') {
            result.catch((err) => { logFailure('subscribe(audio) failed', err); });
          }
        }
      } catch (err) {
        logFailure('setSubscribed(audio) threw', err);
      }
    }

    function resolvePublicationTrack(publication) {
      if (!publication) { return null; }
      const track = publication.track || publication.audioTrack;
      if (track && String(track.kind || '').toLowerCase() === 'audio') { return track; }
      if (track && !track.kind) { return track; }
      return null;
    }

    let voicePlaybackMuted = true;

    function setVoiceAudioMuted(isMuted) {
      voicePlaybackMuted = !!isMuted;

      try {
        const elementsMap = window.__lk_audio_elements || {};
        Object.values(elementsMap).forEach((el) => {
          try {
            if (!el) return;
            el.muted = voicePlaybackMuted;
            if (!voicePlaybackMuted) {
              const maybePlay = el.play?.();
              if (maybePlay && typeof maybePlay.catch === 'function') {
                maybePlay.catch((err) => {
                  try { console.warn('[voice] audio replay blocked', err); } catch {}
                  showVoicePlaybackPrompt();
                });
              }
            }
          } catch {}
        });
      } catch {}
    }

    function showVoicePlaybackPrompt(){
      try {
        const status = document.getElementById('voiceStatus');
        if (status) status.textContent = 'Audio muted. Browser blocked autoplay â€” interact with the page to enable audio.';
      } catch {}
    }

    function hideVoicePlaybackPrompt(){
      try {
        const status = document.getElementById('voiceStatus');
        if (status && status.textContent.toLowerCase().includes('audio muted')) {
          status.textContent = 'Audio enabled. Listening for agent.';
        }
      } catch {}
    }

    async function enableVoicePlayback(){
      const room = window.__roomConn;
      let audioEnabled = false;

      try {
        if (room?.startAudio) {
          room.startAudio().then(() => {
            try { console.log('[voice] room.startAudio resolved'); } catch {}
          }).catch((err) => {
            try { console.warn('[voice] startAudio blocked', err); } catch {}
          });
        }
      } catch (err) {
        try { console.warn('[voice] startAudio threw', err); } catch {}
      }

      try {
        const elements = Object.values(window.__lk_audio_elements || {});
        if (elements.length === 0) {
          // Ensure at least one silent element exists so autoplay can unlock
          const placeholder = document.createElement('audio');
          placeholder.autoplay = true;
          placeholder.playsInline = true;
          placeholder.muted = voicePlaybackMuted;
          placeholder.style.position = 'fixed';
          placeholder.style.bottom = '8px';
          placeholder.style.right = '8px';
          placeholder.style.width = '0px';
          placeholder.style.height = '0px';
          placeholder.style.opacity = '0';
          placeholder.style.pointerEvents = 'none';
          document.body.appendChild(placeholder);
          window.__lk_audio_elements.__placeholder = placeholder;
        }

        const toCheck = Object.values(window.__lk_audio_elements || {});
        toCheck.forEach((el) => {
          try {
            if (!el) return;
            el.autoplay = true;
            el.playsInline = true;
            el.muted = voicePlaybackMuted;
            const maybePlay = el.play?.();
            if (maybePlay && typeof maybePlay.catch === 'function') {
              maybePlay.catch((err) => {
                try { console.warn('[voice] audio play blocked', err); } catch {}
              });
            }
          } catch (playErr) {
            try { console.warn('[voice] audio play invocation failed', playErr); } catch {}
          }
        });
        audioEnabled = true;
      } catch (err) {
        try { console.warn('[voice] audio enable failed', err); } catch {}
      }

      if (audioEnabled) {
        window.__voicePlaybackPrimed = true;
        hideVoicePlaybackPrompt();
        setVoiceAudioMuted(false);
      }
      return audioEnabled;
    }

    function markTranscriptSeen(id){
      if (!id) { return false; }
      const key = String(id);

      // Initialize seen map if needed
      if (!window.__lk_transcript_seen) {
        window.__lk_transcript_seen = {};
      }
      const seenMap = window.__lk_transcript_seen;

      // Check if already seen in either tracking structure
      const alreadySeen = (__seenTranscriptIds && __seenTranscriptIds.has(key)) || seenMap[key];

      // Always mark as seen (even if already was) to ensure consistency
      seenMap[key] = true;
      try { __seenTranscriptIds?.add(key); } catch {}

      return alreadySeen;
    }

    function ingestLivekitTranscript(payload, fallbackRole){
      // Always process LiveKit transcripts - they provide real-time UI updates
      // Supabase Realtime is a backup/sync mechanism, not the primary display source
      try {
        if (!payload) { return; }
        const role = payload.role || fallbackRole || 'assistant';
        const createdAt = payload.createdAt || payload.created_at || new Date().toISOString();
        const text = payload.text || payload.content || payload.transcript || '';
        const citations = payload.citations || [];
        const id = payload.id || `${role}-${createdAt}-${text.slice(0,20)}`;
        if (!text) { return; }
        if (markTranscriptSeen(id)) { return; }
        appendVoiceTranscript(role, text, createdAt, citations, id);
      } catch (err) {
        try { console.warn('[voice] failed to ingest LiveKit transcript', err, payload); } catch {}
      }
    }

    function handleRemoteAudioTrack(track, pub, participant){
      if (!track || track.kind !== 'audio') { return; }
      try { console.log('[voice] handleRemoteAudioTrack', { sid: track.sid, pubSid: pub?.trackSid, participant: participant?.identity }); } catch {}

      try {
        track.setEnabled?.(true);
        track.setVolume?.(1.0);
      } catch {}

      const sidKey = pub?.trackSid || `audio-${track.sid || Date.now()}`;
      try { console.log('[voice] audio sid key', sidKey); } catch {}

      try {
        const previous = track.detach?.() || [];
        previous.forEach((node) => { try { node.remove?.(); } catch {} });
      } catch {}

      let el = window.__lk_audio_elements?.[sidKey];
      try { console.log('[voice] existing audio element?', !!el); } catch {}
      const needAttach = !el || !document.body.contains(el);
      if (needAttach) {
        try { el?.pause?.(); el?.remove?.(); } catch {}
        el = document.createElement('audio');
        el.autoplay = true;
        el.playsInline = true;
        el.muted = voicePlaybackMuted;
        try { el.volume = 1.0; } catch {}
        el.style.position = 'fixed';
        el.style.bottom = '8px';
        el.style.right = '8px';
        el.style.width = '0px';
        el.style.height = '0px';
        el.style.opacity = '0';
        el.style.pointerEvents = 'none';
        document.body.appendChild(el);
        window.__lk_audio_elements[sidKey] = el;
        try { console.log('[voice] created new audio element', sidKey); } catch {}
        if (window.__lk_audio_elements.__placeholder) {
          try { window.__lk_audio_elements.__placeholder.remove?.(); } catch {}
          delete window.__lk_audio_elements.__placeholder;
        }
      }

      try { track.attach(el); } catch {}

      try {
        console.log('[voice] attempting el.play');
        const playPromise = el.play?.();
        if (playPromise && typeof playPromise.then === 'function') {
          playPromise
            .then(() => { try { console.log('[voice] el.play resolved'); } catch {}; })
            .catch((err) => {
              try { console.warn('[voice] audio play blocked', err); } catch {}
              showVoicePlaybackPrompt();
            });
        }
      } catch (err) {
        try { console.warn('[voice] audio play failed', err); } catch {}
        showVoicePlaybackPrompt();
      }

      if (!voicePlaybackMuted) {
        hideVoicePlaybackPrompt();
      }

      try { document.getElementById('voiceStatus').textContent = 'Agent audio connected.'; } catch {}
    }

    function attachExistingRemoteAudio(roomConn){
      try {
        const mapLike = roomConn.remoteParticipants || roomConn.participants || new Map();
        const participants = Array.from(mapLike.values());
        try {
          const debugInfo = participants.map((p) => {
            const details = [];
            iterateTrackPublications(p, (pub, source, key) => {
              details.push({
                from: source,
                key,
                trackSid: pub?.trackSid || pub?.sid,
                source: pub?.source,
                kind: pub?.kind,
                isSubscribed: pub?.isSubscribed,
                hasTrack: !!(pub?.track || pub?.audioTrack)
              });
            });
            return {
              id: p?.identity,
              kind: p?.kind,
              attributes: p?.attributes,
              publications: details
            };
          });
          console.log('[voice] attachExistingRemoteAudio participants', debugInfo);
        } catch (err) {
          console.warn('[voice] attachExistingRemoteAudio logging failed', err);
        }
        participants.forEach((participant) => {
          try {
            const audioPubs = collectAudioPublications(participant);
            const seen = new Set();
            audioPubs.forEach(({ pub, source, key }) => {
              if (!pub) { return; }
              const sid = pub.trackSid || pub.sid || pub.track?.sid;
              if (sid && seen.has(sid)) { return; }
              if (sid) { seen.add(sid); }
              try {
                console.log('[voice] inspect audio pub', {
                  participant: participant?.identity,
                  trackSid: sid,
                  subscribed: pub?.isSubscribed,
                  hasTrack: !!(pub?.track || pub?.audioTrack),
                  kind: pub?.kind,
                  source: pub?.source,
                  via: source,
                  key
                });
              } catch {}
              ensureAudioSubscribed(pub, 'attachExistingRemoteAudio');
              const track = resolvePublicationTrack(pub);
              if (track && track.kind === 'audio') {
                handleRemoteAudioTrack(track, pub, participant);
              } else if (track && !track.kind) {
                handleRemoteAudioTrack(track, pub, participant);
              } else if (pub?.track && String(pub.track.kind || '').toLowerCase() === 'audio') {
                handleRemoteAudioTrack(pub.track, pub, participant);
              }
              if (pub?.on && !pub.__voice_subscribe_hook) {
                try {
                  pub.__voice_subscribe_hook = true;
                  pub.on('subscribed', (track) => {
                    try { console.log('[voice] publication subscribed callback', { trackSid: track?.sid, pubSid: pub?.trackSid }); } catch {}
                    handleRemoteAudioTrack(track, pub, participant);
                  });
                } catch {}
              }
            });
          } catch (perr) {
            try { console.warn('[voice] attachExisting participant failed', perr); } catch {}
          }
        });
      } catch (err) {
        try { console.warn('[voice] attachExistingRemoteAudio error', err); } catch {}
      }
    }
    const urlParams = new URLSearchParams(location.search);
    const SOURCE = urlParams.get('source') || '';
    
    // Initialize citations component
    const citationsComponent = new CitationsComponent();
    function showVoiceTranscriptError(message){
      const container = document.getElementById('voiceTranscriptList');
      if (!container) { return; }
      
      // Check if transcripts already exist
      const hasExistingTranscripts = container.children.length > 0 && 
        !container.querySelector('.text-center.text-red-400');
      
      if (hasExistingTranscripts) {
        // Don't clear existing transcripts, just add a banner at the top
        const existingBanner = document.getElementById('voice-transcript-error-banner');
        if (existingBanner) {
          return; // Banner already exists
        }
        
        const banner = document.createElement('div');
        banner.id = 'voice-transcript-error-banner';
        banner.className = 'mb-2 p-2 text-xs text-orange-400 border border-orange-500/30 rounded-md bg-orange-500/5';
        banner.innerHTML = `
          <p class="font-semibold mb-1">âš ï¸ Live updates paused</p>
          <p class="text-xs">${message}</p>
        `;
        container.insertBefore(banner, container.firstChild);
      } else {
        // No transcripts yet, show full error message
        container.innerHTML = `
          <div class="text-center text-red-400 text-sm py-4 border border-red-500/30 rounded-md bg-red-500/5">
            <p class="font-semibold mb-1">Live transcripts unavailable</p>
            <p>${message}</p>
          </div>
        `;
      }
    }
function clearVoiceTranscriptError(){
  const container = document.getElementById('voiceTranscriptList');
  if (!container) { return; }
  const banner = document.getElementById('voice-transcript-error-banner');
  if (banner) {
    banner.remove();
  }
  const standalone = container.querySelector('.text-center.text-red-400');
  if (standalone && standalone.parentElement === container) {
    container.removeChild(standalone);
    container.innerHTML = '';
  }
}
    let storedVoiceSession = null;
    let connectionMonitor = null;
    let monitorFailures = 0;
    let reconnectLock = false;
    let reconnectAttempts = 0;
    let manualVoiceEnd = false;
    let activeConversationId = null;
    let pendingTranscriptStart = null;
    function getTranscriptSupabase(){
      if (transcriptSupabase) { return transcriptSupabase; }
      if (!window.supabase) {
        try { console.warn('[voice] Supabase client library missing'); } catch {}
        showVoiceTranscriptError('Supabase client library not available.');
        return null;
      }
      if (!CLIENT_SUPABASE_URL || !CLIENT_SUPABASE_ANON) {
        try { console.warn('[voice] transcript Supabase config missing'); } catch {}
        showVoiceTranscriptError('Client Supabase credentials are missing. Please contact support.');
        return null;
      }
      transcriptSupabase = supabase.createClient(CLIENT_SUPABASE_URL, CLIENT_SUPABASE_ANON);
      if (CLIENT_SUPABASE_BOOTSTRAP.access_token) {
        transcriptSupabase.auth.setSession({
          access_token: CLIENT_SUPABASE_BOOTSTRAP.access_token,
          refresh_token: CLIENT_SUPABASE_BOOTSTRAP.refresh_token || null
        }).catch((err) => {
          console.warn('[voice] failed to bootstrap client Supabase session', err);
          showVoiceTranscriptError('Unable to authenticate with client Supabase. Please refresh and try again.');
        });
      }
      return transcriptSupabase;
    }
    async function ensureTranscriptSupabaseAuth(){
      const clientSb = getTranscriptSupabase();
      if (!clientSb) { return false; }
      try {
        // Check if we have an authenticated session
        const { data } = await clientSb.auth.getSession();
        if (data && data.session) { return true; }
        
        // Try to set session if bootstrap tokens are available
        if (CLIENT_SUPABASE_BOOTSTRAP.access_token) {
          const { data: setData, error } = await clientSb.auth.setSession({
            access_token: CLIENT_SUPABASE_BOOTSTRAP.access_token,
            refresh_token: CLIENT_SUPABASE_BOOTSTRAP.refresh_token || null
          });
          if (!error && setData?.session) { return true; }
        }
        
        // For public embeds: anon key authentication is sufficient
        // RLS policies allow anon users to read transcripts
        // Just return true to proceed with anon authentication
        try { console.info('[voice] using anon authentication for transcripts'); } catch {}
        return true;
      } catch (err) {
        try { console.warn('[voice] transcript auth check failed', err); } catch {}
        // Still return true for anon auth
        return true;
      }
    }

    async function ensureSession() {
      if (manualToken) { return true; }
      const { data } = await sb.auth.getSession();
      session = data?.session || null;
      if (session && !CLIENT_SUPABASE_BOOTSTRAP.access_token) {
        await ensureTranscriptSupabaseAuth();
      }
      return !!session;
    }
    function setAuthVisible(v) { document.getElementById('auth').classList.toggle('hidden', !v); }
    function setChatVisible(v) { document.getElementById('chat').classList.toggle('hidden', !v); }

    async function syncClientUserCredentials(email, password){
      try {
        const token = manualToken || (session && session.access_token);
        if (!token) {
          console.warn('[voice] unable to sync client user without platform token');
          return false;
        }
        const resp = await fetch('/api/embed/client-users/sync', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${token}`
          },
          body: JSON.stringify({
            client_id: CLIENT_ID,
            email,
            password
          })
        });
        if (!resp.ok) {
          console.warn('[voice] sync client user failed', await resp.text());
          return false;
        }
        return true;
      } catch (err) {
        console.warn('[voice] sync client user exception', err);
        return false;
      }
    }

    async function ensureClientSupabaseSession(email, password){
      const clientSb = getTranscriptSupabase();
      if (!clientSb || !email || !password) { return false; }
      try {
        const { data } = await clientSb.auth.getSession();
        if (data && data.session) { return true; }
      } catch {}
      const signInResponse = await clientSb.auth.signInWithPassword({ email, password });
      if (signInResponse.error) {
        const errorMessage = (signInResponse.error.message || '').toLowerCase();
        if (errorMessage.includes('invalid login') || errorMessage.includes('not found')) {
          const synced = await syncClientUserCredentials(email, password);
          if (!synced) { throw signInResponse.error; }
          const retry = await clientSb.auth.signInWithPassword({ email, password });
          if (retry.error) { throw retry.error; }
        } else {
          throw signInResponse.error;
        }
      }
      return true;
    }

    async function login() {
      const email = document.getElementById('email').value.trim();
      const password = document.getElementById('password').value;
      const res = await sb.auth.signInWithPassword({ email, password });
      if (res.error) { document.getElementById('auth-msg').innerText = res.error.message; return; }
      if (res.data?.session) {
        session = res.data.session;
      }
      try {
        await ensureClientSupabaseSession(email, password);
      } catch (err) {
        console.warn('[voice] client Supabase login failed', err);
        document.getElementById('auth-msg').innerText = 'Unable to access voice transcripts. Please try again or contact support.';
        return;
      }
      await ensureSession(); setAuthVisible(false); setChatVisible(true);
    }
    async function signup() {
      const email = document.getElementById('email').value.trim();
      const password = document.getElementById('password').value;
      const res = await sb.auth.signUp({ email, password });
      if (res.error) { document.getElementById('auth-msg').innerText = res.error.message; return; }
      const synced = await syncClientUserCredentials(email, password);
      if (!synced) {
        document.getElementById('auth-msg').innerText = 'Account created, but voice access could not be provisioned automatically.';
        return;
      }
      if (res.data?.session) {
        session = res.data.session;
        await ensureClientSupabaseSession(email, password);
      }
      document.getElementById('auth-msg').innerText = 'Check your email to confirm.';
    }

    function switchTab(t) {
      // Prevent switching to disabled modes - show tooltip instead
      if (t === 'voice' && !VOICE_CHAT_ENABLED) {
        showDisabledModeTooltip('voice');
        return;
      }
      if (t === 'text' && !TEXT_CHAT_ENABLED) {
        showDisabledModeTooltip('text');
        return;
      }
      if (t === 'video' && !VIDEO_CHAT_ENABLED) {
        showDisabledModeTooltip('video');
        return;
      }

      const textBtn = document.getElementById('tab-text');
      const voiceBtn = document.getElementById('tab-voice');
      const videoBtn = document.getElementById('tab-video');
      const panelText = document.getElementById('panel-text');
      const panelVoice = document.getElementById('panel-voice');
      const panelVideo = document.getElementById('panel-video');
      
      // Reset all tabs and panels
      textBtn.classList.remove('active');
      voiceBtn.classList.remove('active');
      if (videoBtn) videoBtn.classList.remove('active');
      panelText.classList.add('hidden');
      panelVoice.classList.add('hidden');
      if (panelVideo) panelVideo.classList.add('hidden');
      
      // Activate selected tab and panel
      if (t === 'text') {
        textBtn.classList.add('active');
        panelText.classList.remove('hidden');
      } else if (t === 'voice') {
        voiceBtn.classList.add('active');
        panelVoice.classList.remove('hidden');
      } else if (t === 'video' && videoBtn && panelVideo) {
        videoBtn.classList.add('active');
        panelVideo.classList.remove('hidden');
      }
    }

    function showDisabledModeTooltip(mode) {
      const modeName = mode === 'voice' ? 'Voice chat' : (mode === 'video' ? 'Video chat' : 'Text chat');
      const tooltip = document.getElementById('disabledModeTooltip');
      const tooltipText = document.getElementById('disabledModeTooltipText');
      if (tooltip && tooltipText) {
        tooltipText.textContent = `${modeName} is not available for this sidekick`;
        tooltip.classList.remove('hidden');
        tooltip.classList.add('animate-fade-in');
        setTimeout(() => {
          tooltip.classList.add('hidden');
          tooltip.classList.remove('animate-fade-in');
        }, 2500);
      }
    }

    function initChatModeUI() {
      const modeToggle = document.querySelector('.mode-toggle');
      const textBtn = document.getElementById('tab-text');
      const voiceBtn = document.getElementById('tab-voice');
      const videoBtn = document.getElementById('tab-video');

      // Style disabled modes as grayed out instead of hiding them
      if (!TEXT_CHAT_ENABLED && textBtn) {
        textBtn.classList.add('disabled-mode');
        textBtn.setAttribute('title', 'Text chat is not available for this sidekick');
      }
      if (!VOICE_CHAT_ENABLED && voiceBtn) {
        voiceBtn.classList.add('disabled-mode');
        voiceBtn.setAttribute('title', 'Voice chat is not available for this sidekick');
      }
      if (!VIDEO_CHAT_ENABLED && videoBtn) {
        videoBtn.classList.add('disabled-mode');
        videoBtn.setAttribute('title', 'Video chat is not available for this sidekick');
      }

      // Set default tab based on which modes are enabled
      if (TEXT_CHAT_ENABLED) {
        switchTab('text');
      } else if (VOICE_CHAT_ENABLED) {
        switchTab('voice');
      } else if (VIDEO_CHAT_ENABLED) {
        switchTab('video');
      }
    }

    function addUser(text, shouldScroll = true) {
      const chatMessages = document.getElementById('chatMessages');
      const welcome = document.getElementById('welcomeMessage');
      if (welcome) welcome.remove();

      const wrap = document.createElement('div'); wrap.className='flex justify-end';
      wrap.innerHTML = `<div class="max-w-[80%] px-4 py-3 rounded-2xl msg-user text-white">${escapeAndLinkify(text)}</div>`;
      chatMessages.appendChild(wrap);
      if (shouldScroll) scrollBottom();
    }
    function addAsstHtml(html) {
      const wrap = document.createElement('div'); wrap.className='flex justify-start';
      const b = document.createElement('div'); b.className='max-w-[80%] px-4 py-3 rounded-2xl msg-assistant text-white/90 whitespace-pre-wrap';
      b.innerHTML = html; wrap.appendChild(b);
      document.getElementById('chatMessages').appendChild(wrap);
      scrollBottom();
    }
    // Smart scroll - only auto-scroll if user is near bottom (within 150px)
    function scrollBottom(force = false){
      const el = document.getElementById('chatMessages');
      if (!el) return;
      const isNearBottom = (el.scrollHeight - el.scrollTop - el.clientHeight) < 150;
      if (force || isNearBottom) {
        el.scrollTop = el.scrollHeight;
      }
    }

    async function sendMsg(evt){
      if (evt && evt.preventDefault) evt.preventDefault();
      const ok = await ensureSession(); if (!ok) { setAuthVisible(true); setChatVisible(false); return false; }
      const input = document.getElementById('msg');
      const text = input.value; if (!text) return false;
      input.value = '';

      // Track if this is the first message in a new conversation
      const wasFirstMessage = isFirstMessage;
      isFirstMessage = false;

      addUser(text);
      incrementSessionMessageCount(); // Track for ambient abilities
      // Insert loading indicator like preview
      const chatMessages = document.getElementById('chatMessages');
      const loadingDiv = document.createElement('div');
      loadingDiv.className = 'flex justify-start';
      loadingDiv.id = 'tempLoadingIndicator';
      loadingDiv.innerHTML = `<div class="max-w-[80%] px-4 py-3 rounded-2xl msg-assistant"><div class="flex space-x-2"><div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:0ms"></div><div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:150ms"></div><div class="w-2 h-2 bg-brand-teal rounded-full animate-bounce" style="animation-delay:300ms"></div></div></div>`;
      chatMessages.appendChild(loadingDiv);

      // stream (FormData to match server Form(...))
      const fd = new FormData();
      fd.append('client_id', CLIENT_ID);
      fd.append('agent_slug', AGENT_SLUG);
      fd.append('message', text);

      // Include conversation_id if continuing an existing conversation
      if (currentConversationId) {
        fd.append('conversation_id', currentConversationId);
      }

      // Include user_id for conversation tracking
      const userId = getEffectiveUserId();
      if (userId) {
        fd.append('user_id', userId);
      }

      const bearer = manualToken || (session && session.access_token) || '';
      const res = await fetch('/api/embed/text/stream', { method: 'POST', body: fd, headers: { Authorization: `Bearer ${bearer}` } });
      if (!res.ok || !res.body) {
        const errDiv = document.createElement('div'); errDiv.className='flex justify-start';
        errDiv.innerHTML = `<div class="w-3/4 px-4 py-2 rounded-lg bg-dark-elevated text-dark-text border border-dark-border">Sorry, streaming failed.</div>`;
        try { chatMessages.replaceChild(errDiv, loadingDiv); } catch { chatMessages.appendChild(errDiv); }
        return false;
      }
      const reader = res.body.getReader(); const decoder = new TextDecoder(); let buf='';
      let acc = '';
      const asstDiv = document.createElement('div'); asstDiv.className='flex flex-col items-start';
      const asstBubble = document.createElement('div'); asstBubble.className='max-w-[80%] px-4 py-3 rounded-2xl msg-assistant text-white/90 whitespace-pre-wrap'; asstBubble.textContent=''; asstDiv.appendChild(asstBubble);
      let inserted = false;
      while(true){
        const {value, done} = await reader.read(); if (done) break;
        buf += decoder.decode(value, {stream:true}); buf = buf.replace(/\r\n/g,'\n');
        const parts = buf.split('\n\n'); buf = parts.pop();
        for(const p of parts){ if(!p.startsWith('data: ')) continue; try{
          const payload = JSON.parse(p.slice(6));
          if (payload.error) {
            if (!inserted){ chatMessages.replaceChild(asstDiv, loadingDiv); inserted = true; }
            const msg = payload.error === 'unauthorized' ? 'Please sign in to chat.' : 'Sorry, streaming failed.';
            asstBubble.textContent = msg;
            if (payload.error === 'unauthorized') { setAuthVisible(true); setChatVisible(false); }
          }
          if(payload.delta){
            if (!inserted){ chatMessages.replaceChild(asstDiv, loadingDiv); inserted = true; }
            acc += String(payload.delta).replace(/\\n/g,'\n');
            asstBubble.innerHTML = (window.marked && window.DOMPurify) ? DOMPurify.sanitize(marked.parse(acc)) : acc;
          }
          if(payload.done && payload.full_text){
            if (!inserted){ chatMessages.replaceChild(asstDiv, loadingDiv); inserted = true; }
            acc = String(payload.full_text).replace(/\\n/g,'\n');
            asstBubble.innerHTML = (window.marked && window.DOMPurify) ? DOMPurify.sanitize(marked.parse(acc)) : acc;

            // DEBUG: Log what we received from the backend
            console.log('[stream] DONE received - full_text length:', payload.full_text?.length, 'preview:', payload.full_text?.slice(0, 100));
            console.log('[stream] DONE received - citations count:', payload.citations?.length || 0);
            console.log('[stream] DONE received - widget present:', !!payload.widget, payload.widget);
            console.log('[stream] DONE received - all payload keys:', Object.keys(payload));
            if (payload.citations && payload.citations.length > 0) {
              console.log('[stream] Citation titles:', payload.citations.slice(0, 3).map(c => c.title));
            }

            // Update conversation_id from response
            if (payload.conversation_id) {
              currentConversationId = payload.conversation_id;
              console.log('[history] Conversation ID:', currentConversationId, 'is_new:', payload.is_new_conversation);

              // Generate title for new conversations
              if (payload.is_new_conversation && wasFirstMessage) {
                generateConversationTitle(currentConversationId, text);
              }
            }

            // Handle widget triggers if present (before citations - widgets take priority)
            let widgetCreated = false;
            if (payload.widget) {
              console.log('[stream] Widget trigger received:', payload.widget);
              try {
                widgetCreated = handleWidgetTrigger(payload.widget);
                if (widgetCreated) {
                  console.log('[stream] Widget created successfully, skipping citations');
                } else {
                  console.warn('[stream] Widget creation returned null');
                }
              } catch (widgetErr) {
                console.error('[stream] Widget creation error:', widgetErr);
              }
            }

            // Handle citations only if no widget was created
            if (!widgetCreated && payload.citations && payload.citations.length > 0) {
              const messageId = `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
              const citationsElement = citationsComponent.render(payload.citations, messageId);
              if (citationsElement) {
                // Add compact class for embed context
                citationsElement.classList.add('compact');
                asstDiv.appendChild(citationsElement);
                scrollBottom();
              }
            }
          }
        }catch{}}
      }
      // If stream ended without any insertion, show a fallback message
      if (!inserted) {
        const errDiv = document.createElement('div'); errDiv.className='flex justify-start';
        errDiv.innerHTML = `<div class="max-w-[80%] px-4 py-3 rounded-2xl msg-assistant text-white/90">No response received.</div>`;
        try { chatMessages.replaceChild(errDiv, loadingDiv); } catch { chatMessages.appendChild(errDiv); }
      }
      return false;
    }

    let __roomConn = null;
    let __seenTranscriptIds = new Set();
    function updateVoiceTranscript(id, text, role) {
      try {
        if (!id) return;
        const existing = document.querySelector(`[data-transcript-id="${id}"]`);
        if (existing) {
          const bubble = existing.querySelector('[data-transcript-text]');
          if (bubble) {
            const isUser = role === 'user';
            const newText = text || '';

            // For user messages: only update if new text is longer
            // This prevents out-of-order STT updates from overwriting the full message
            if (isUser) {
              const currentText = bubble.textContent || '';
              if (newText.length >= currentText.length) {
                bubble.innerHTML = escapeAndLinkify(newText);
              }
              // else: skip update - probably an out-of-order interim result
            } else {
              // For assistant messages: always update with markdown rendering
              // Debug: log what we're receiving
              console.log('[voice] UPDATE raw text has newlines:', newText.includes('\n'), 'double:', newText.includes('\n\n'));
              const rawText = newText.replace(/\\n/g, '\n');
              console.log('[voice] UPDATE after replace has newlines:', rawText.includes('\n'), 'double:', rawText.includes('\n\n'));
              bubble.innerHTML = (window.marked && window.DOMPurify)
                ? DOMPurify.sanitize(marked.parse(rawText))
                : rawText;
            }
            // Smart auto-scroll - only if user is near bottom
            const container = document.getElementById('voiceTranscriptList');
            if (container) {
              const isNearBottom = (container.scrollHeight - container.scrollTop - container.clientHeight) < 150;
              if (isNearBottom) container.scrollTop = container.scrollHeight;
            }
          }
        }
      } catch (err) {
        try { console.warn('[voice] failed to update transcript', err); } catch {}
      }
    }

    // Content-based deduplication for transcripts (catches duplicates from different sources)
    let __recentTranscriptTexts = [];
    function isRecentDuplicateText(role, text) {
      if (!text || text.length < 5) return false;
      const normalizedText = text.trim().toLowerCase().slice(0, 100);
      const key = `${role}:${normalizedText}`;
      if (__recentTranscriptTexts.includes(key)) return true;
      __recentTranscriptTexts.push(key);
      // Keep only last 10 entries to avoid memory bloat
      if (__recentTranscriptTexts.length > 10) __recentTranscriptTexts.shift();
      return false;
    }

    function appendVoiceTranscript(role, text, createdAt, citations, id){
      try {
        if (id && markTranscriptSeen(id)) {
          try { console.log('[voice] appendVoiceTranscript SKIPPED (already seen by ID):', id?.slice(0,8), role); } catch {}
          return;
        }
        // Content-based deduplication - catches same text from different sources (LiveKit vs Supabase)
        if (isRecentDuplicateText(role, text)) {
          try { console.log('[voice] appendVoiceTranscript SKIPPED (duplicate text):', id?.slice(0,8), role, text?.slice(0,30)); } catch {}
          if (id) markTranscriptSeen(id); // Still mark ID as seen
          return;
        }
        try { console.log('[voice] appendVoiceTranscript ADDING:', id?.slice(0,8), role, text?.slice(0,30)); } catch {}
        const container = document.getElementById('voiceTranscriptList');
        if (!container) return;
        const wrap = document.createElement('div');
        const isUser = role === 'user';
        wrap.className = isUser ? 'flex justify-end' : 'flex justify-start';
        if (id) wrap.setAttribute('data-transcript-id', id); // Add ID for updating
        const bubble = document.createElement('div');
        bubble.setAttribute('data-transcript-text', 'true'); // Add attribute for finding text element
        bubble.className = (isUser
          ? 'max-w-[80%] px-4 py-3 rounded-2xl msg-user text-white'
          : 'max-w-[80%] px-4 py-3 rounded-2xl msg-assistant text-white/90 prose prose-invert prose-sm max-w-none');
        // Render markdown for assistant messages, linkify URLs for user messages
        if (isUser) {
          bubble.innerHTML = escapeAndLinkify(text || '');
        } else {
          // Use marked + DOMPurify for markdown rendering (same as text mode)
          // Debug: log what we're receiving
          console.log('[voice] INSERT raw text has newlines:', (text || '').includes('\n'), 'double:', (text || '').includes('\n\n'));
          const rawText = (text || '').replace(/\\n/g, '\n');
          console.log('[voice] INSERT after replace has newlines:', rawText.includes('\n'), 'double:', rawText.includes('\n\n'));
          bubble.innerHTML = (window.marked && window.DOMPurify)
            ? DOMPurify.sanitize(marked.parse(rawText))
            : rawText;
        }
        wrap.appendChild(bubble);
        container.appendChild(wrap);
        if (citations && Array.isArray(citations) && citations.length > 0) {
          const citationsElement = citationsComponent.render(citations, `voice-${Date.now()}`);
          if (citationsElement) {
            citationsElement.classList.add('compact');
            const citeWrap = document.createElement('div');
            citeWrap.className = 'flex justify-start';
            citeWrap.appendChild(citationsElement);
            container.appendChild(citeWrap);
          }
        }
        // Smart auto-scroll - only if user is near bottom
        const isNearBottom = (container.scrollHeight - container.scrollTop - container.clientHeight) < 150;
        if (isNearBottom) container.scrollTop = container.scrollHeight;
        // Note: markTranscriptSeen already added to __seenTranscriptIds, no need to duplicate
      } catch {}
    }

    let _transcriptStreamStarting = false; // Guard against concurrent calls
    async function startVoiceTranscriptStream(conversationId, clientId, agentId){
      try {
        // Skip if already starting for same conversation (prevents race condition duplicates)
        if (_transcriptStreamStarting && transcriptConversationId === conversationId) {
          try { console.log('[voice] startVoiceTranscriptStream SKIPPED (already starting):', conversationId?.slice(0,8)); } catch {}
          return;
        }
        if (!conversationId || !clientId) {
          try { console.warn('[voice] missing conversation/client id for transcript stream'); } catch {}
          showVoiceTranscriptError('Conversation metadata missing; transcripts cannot be loaded.');
          return;
        }
        try { console.log('[voice] startVoiceTranscriptStream BEGIN:', conversationId?.slice(0,8)); } catch {}
        _transcriptStreamStarting = true;
        const supabaseClient = getTranscriptSupabase();
        if (!supabaseClient) { _transcriptStreamStarting = false; return; }
        const authorized = await ensureTranscriptSupabaseAuth();
        if (!authorized) {
          try { console.warn('[voice] transcript Supabase auth unavailable'); } catch {}
          pendingTranscriptStart = { conversationId, clientId, agentId };
          _transcriptStreamStarting = false;
          return;
        }
        pendingTranscriptStart = null;
        if (transcriptConversationId !== conversationId) {
          // Clear both deduplication trackers when switching conversations
          __seenTranscriptIds?.clear?.();
          window.__lk_transcript_seen = {};
        }
        transcriptConversationId = conversationId;
        stopVoiceTranscriptStream();
        transcriptRealtimeChannel = supabaseClient
          .channel(`voice-transcripts:${conversationId}`)
          .on(
            'postgres_changes',
            {
              event: 'INSERT',
              schema: 'public',
              table: 'conversation_transcripts',
              filter: `conversation_id=eq.${conversationId}`
            },
            (payload) => {
              const row = payload?.new;
              if (!row) { return; }
              const txt = row.content || row.transcript || '';
              if (!txt) { return; }
              const alreadySeen = __seenTranscriptIds?.has(row.id) || window.__lk_transcript_seen?.[row.id];
              try { console.log('[voice] INSERT event:', row.id?.slice(0,8), row.role, 'len:', txt?.length, 'seen:', alreadySeen); } catch {}
              if (!alreadySeen) {
                appendVoiceTranscript(row.role, txt, row.created_at, row.citations, row.id);
              } else {
                try { console.log('[voice] INSERT SKIPPED (pre-check seen):', row.id?.slice(0,8)); } catch {}
              }
            }
          )
          .on(
            'postgres_changes',
            {
              event: 'UPDATE',
              schema: 'public',
              table: 'conversation_transcripts',
              filter: `conversation_id=eq.${conversationId}`
            },
            (payload) => {
              const row = payload?.new;
              if (!row) { return; }
              const txt = row.content || row.transcript || '';
              if (!txt) { return; }
              try { console.log('[voice] UPDATE event:', row.id?.slice(0,8), row.role, 'len:', txt?.length); } catch {}
              updateVoiceTranscript(row.id, txt, row.role);
            }
          )
          .subscribe((status) => {
            if (status === 'SUBSCRIBED') {
              try { console.log('[voice] realtime subscribed for transcripts (INSERT + UPDATE)'); } catch {}
            } else if (status === 'CHANNEL_ERROR') {
              try { console.warn('[voice] transcript channel error; retrying'); } catch {}
              setTimeout(() => {
                if (transcriptConversationId === conversationId) {
                  startVoiceTranscriptStream(conversationId, clientId, agentId);
                }
              }, 2000);
            }
          });
        await loadVoiceTranscriptHistory(conversationId);
        clearVoiceTranscriptError();
        try { console.log('[voice] startVoiceTranscriptStream COMPLETE:', conversationId?.slice(0,8)); } catch {}
      } catch (err) {
        try { console.warn('[voice] failed to start transcript stream', err); } catch {}
        showVoiceTranscriptError('Unable to start transcript stream. Please refresh and try again.');
      } finally {
        _transcriptStreamStarting = false;
      }
    }

    async function loadVoiceTranscriptHistory(conversationId){
      const supabaseClient = getTranscriptSupabase();
      if (!supabaseClient || !conversationId) { return; }
      const authorized = await ensureTranscriptSupabaseAuth();
      if (!authorized) { return; }
      try {
        try { console.log('[voice] loadVoiceTranscriptHistory START for', conversationId?.slice(0,8)); } catch {}
        const { data, error } = await supabaseClient
          .from('conversation_transcripts')
          .select('*')
          .eq('conversation_id', conversationId)
          .order('created_at', { ascending: true })
          .limit(200);
        if (error) {
          try { console.warn('[voice] transcript history error', error); } catch {}
          showVoiceTranscriptError('Unable to load transcript history. Please refresh to try again.');
          return;
        }
        try { console.log('[voice] loadVoiceTranscriptHistory got', data?.length || 0, 'rows'); } catch {}

        // Pre-mark all history IDs as seen FIRST to prevent race conditions
        // with realtime events that might arrive during the forEach loop
        (data || []).forEach((t) => {
          if (t.id) {
            markTranscriptSeen(t.id);
          }
        });

        // Now append the transcripts to the DOM
        (data || []).forEach((t) => {
          const txt = t.content || t.transcript || '';
          if (txt) {
            try { console.log('[voice] history row:', t.id?.slice(0,8), t.role, 'len:', txt?.length); } catch {}
            // Directly append without re-checking markTranscriptSeen since we just marked them
            const container = document.getElementById('voiceTranscriptList');
            if (!container) return;
            const wrap = document.createElement('div');
            const isUser = t.role === 'user';
            wrap.className = isUser ? 'flex justify-end' : 'flex justify-start';
            if (t.id) wrap.setAttribute('data-transcript-id', t.id);
            const bubble = document.createElement('div');
            bubble.setAttribute('data-transcript-text', 'true');
            bubble.className = (isUser
              ? 'max-w-[80%] px-4 py-3 rounded-2xl msg-user text-white'
              : 'max-w-[80%] px-4 py-3 rounded-2xl msg-assistant text-white/90 prose prose-invert prose-sm max-w-none');
            if (isUser) {
              bubble.innerHTML = escapeAndLinkify(txt);
            } else {
              const rawText = txt.replace(/\\n/g, '\n');
              bubble.innerHTML = (window.marked && window.DOMPurify)
                ? DOMPurify.sanitize(marked.parse(rawText))
                : rawText;
            }
            wrap.appendChild(bubble);
            container.appendChild(wrap);
            // Handle citations
            if (t.citations && Array.isArray(t.citations) && t.citations.length > 0) {
              const citationsElement = citationsComponent.render(t.citations, `voice-hist-${t.id}`);
              if (citationsElement) {
                citationsElement.classList.add('compact');
                const citeWrap = document.createElement('div');
                citeWrap.className = 'flex justify-start';
                citeWrap.appendChild(citationsElement);
                container.appendChild(citeWrap);
              }
            }
          }
        });

        // Scroll to bottom after loading history
        const container = document.getElementById('voiceTranscriptList');
        if (container) container.scrollTop = container.scrollHeight;
        if (data && data.length > 0) {
          clearVoiceTranscriptError();
        }
      } catch (err) {
        try { console.warn('[voice] transcript history fetch failed', err); } catch {}
        showVoiceTranscriptError('Unable to load transcript history. Please refresh to try again.');
      }
    }

    function stopVoiceTranscriptStream(){
      try {
        if (transcriptRealtimeChannel && transcriptSupabase) {
          transcriptSupabase.removeChannel(transcriptRealtimeChannel);
        }
      } catch (err) {
        try { console.warn('[voice] error while stopping transcript stream', err); } catch {}
      }
      transcriptRealtimeChannel = null;
    }

    function clearVoiceConnectionMonitor() {
      if (connectionMonitor) {
        clearInterval(connectionMonitor);
        connectionMonitor = null;
      }
    }

    function scheduleVoiceConnectionMonitor(roomConn) {
      clearVoiceConnectionMonitor();
      connectionMonitor = setInterval(() => {
        try {
          if (!roomConn || manualVoiceEnd) {
            monitorFailures = 0;
            return;
          }
          const state = roomConn.state;
          const participantsMap = roomConn.remoteParticipants || roomConn.participants || new Map();
          const participants = Array.from(participantsMap.values());
          const agentPresent = participants.some((rp) => String(rp.identity || '').toLowerCase().includes('agent'));

          const connectionIssue = state !== 'connected';
          const agentMissing = !agentPresent;

          if (connectionIssue || agentMissing) {
            monitorFailures += 1;
            console.warn('[voice] connection monitor detected issue', { state, agentPresent, monitorFailures });
          } else {
            monitorFailures = 0;
          }

          if (!manualVoiceEnd && connectionIssue && monitorFailures >= 3) {
            monitorFailures = 0;
            tryVoiceReconnect('monitor');
          }
        } catch (err) {
          console.warn('[voice] monitor error', err);
        }
      }, 8000);
    }

    async function tryVoiceReconnect(reason) {
      if (manualVoiceEnd) {
        console.log('[voice] reconnect skipped (manual end)');
        return;
      }
      if (!storedVoiceSession) {
        console.log('[voice] reconnect skipped (no stored session)');
        return;
      }
      if (reconnectLock) {
        return;
      }
      reconnectLock = true;
      reconnectAttempts += 1;
      if (reconnectAttempts > 5) {
        console.error('[voice] reconnect aborted after max attempts');
        reconnectLock = false;
        return;
      }
      console.log(`[voice] attempting reconnect (${reason}) attempt ${reconnectAttempts}`);
      try {
        const status = document.getElementById('voiceStatus');
        if (status) status.textContent = 'Reconnecting voiceâ€¦';
        if (__roomConn) {
          try { await __roomConn.disconnect(); } catch {}
        }
        await new Promise((resolve) => setTimeout(resolve, 600));
        await connectLivekit(storedVoiceSession, true);
      } catch (err) {
        console.error('[voice] reconnect failed', reason, err);
        reconnectLock = false;
        setTimeout(() => tryVoiceReconnect(reason), 2000);
        return;
      }
      reconnectLock = false;
    }

    // Removed overlay in favor of autoplay under user gesture

    async function startVoice(){
      try { console.log('[voice] startVoice invoked'); } catch {}

      // With inline paywall, users can only reach this function after purchase
      // Double-check entitlement for safety
      if (SUPERTAB_CONFIG.enabled && SUPERTAB_CONFIG.clientId && SUPERTAB_CONFIG.experienceId) {
        if (!hasVoiceEntitlement) {
          console.log('[voice] No entitlement - user should use purchase button first');
          return;
        }
      }

      const status = document.getElementById('voiceStatus');
      setVoiceAudioMuted(true);
      showVoicePlaybackPrompt();
      try {
        const existingEls = window.__lk_audio_elements || {};
        Object.entries(existingEls).forEach(([key, el]) => {
          try { el?.remove?.(); } catch {}
          delete existingEls[key];
        });
        window.__lk_audio_elements = existingEls;
      } catch {}
      try {
        window.__lk_transcript_seen = {};
        __seenTranscriptIds = new Set();
        __recentTranscriptTexts = [];
      } catch {}
      let playbackPrimed = false;
      try {
        playbackPrimed = await enableVoicePlayback();
        try { console.log('[voice] enableVoicePlayback result', playbackPrimed); } catch {}
      } catch (err) {
        try { console.warn('[voice] initial enableVoicePlayback failed', err); } catch {}
      }

      // Allow either a Supabase session OR a manual token from admin preview
      try { console.log('[voice] ensuring session'); } catch {}
      const ok = await ensureSession();
      try { console.log('[voice] ensureSession complete', { ok, manualToken: !!manualToken, sessionUser: session?.user?.id }); } catch {}
      const injectedUserId = window.__client_user_id || null;
      const hasSupabaseUser = !!(session && session.user && session.user.id);
      const hasManualToken = !!manualToken;
      try { console.log('[voice] auth status', { hasSupabaseUser, hasManualToken, injectedUserId }); } catch {}
      if (!hasSupabaseUser && !hasManualToken) {
        setAuthVisible(true); setChatVisible(false);
        document.getElementById('voiceStatus').textContent = 'Please sign in to start voice.';
        try { console.warn('[voice] abort startVoice - no session or manual token'); } catch {}
        return;
      }
      const room = `embed_${AGENT_SLUG}_${Date.now()}`;
      const payload = {
        agent_slug: AGENT_SLUG,
        client_id: CLIENT_ID,
        mode: 'voice',
        room_name: room,
        platform: 'livekit',
        user_id: (hasSupabaseUser ? session.user.id : (injectedUserId || 'preview-user'))
      };
      const bearer = manualToken || (session && session.access_token) || '';
      try { console.log('[voice] trigger payload prepared', { room, bearerPresent: !!bearer }); } catch {}
      const res = await fetch('/api/v1/trigger-agent', { method: 'POST', headers: { 'Content-Type':'application/json', Authorization: `Bearer ${bearer}` }, body: JSON.stringify(payload) });
      try { console.log('[voice] trigger request sent', payload); } catch {}
      const startBtn = document.getElementById('voiceStartBtn');
      const endBtn = document.getElementById('voiceEndBtn');
      if (!playbackPrimed && !window.__voicePlaybackPrimed) {
        if (status) status.textContent = 'Audio muted â€” browser blocked autoplay. Interact with the page to enable audio.';
        try { await enableVoicePlayback(); } catch {}
      }
      if (!res.ok) { status.textContent = 'Voice start failed.'; return; }
      const data = await res.json();
      try { console.log('[voice] trigger response', data); } catch {}
      if (!data || !data.data) { status.textContent = 'Error: trigger-agent returned no data payload.'; console.log('[voice] trigger payload (raw)', data); return; }
      const info = data.data.livekit_config; const server = info?.server_url; const token = info?.user_token;
      const agentId = (data?.data?.agent_context?.agent_id || data?.agent_context?.agent_id || '');
      // Enforce no-fallback: require top-level conversation_id only
      const conversationId = data?.data?.conversation_id || null;
      try { console.log('[voice] trigger keys', Object.keys(data||{})); } catch {}
      try { console.log('[voice] conversationId', conversationId); } catch {}
      if (!conversationId) { status.textContent = 'Error: missing conversation_id from trigger-agent.'; return; }
      if (!server || !token) { status.textContent = 'Voice start response incomplete.'; return; }
      if (conversationId && conversationId !== activeConversationId) {
        activeConversationId = conversationId;
        startVoiceTranscriptStream(conversationId, CLIENT_ID, agentId);
        // Ensure transcript list is visible when voice starts (may be hidden if not using Supertab)
        const transcriptList = document.getElementById('voiceTranscriptList');
        if (transcriptList) transcriptList.classList.remove('hidden');
      }
      storedVoiceSession = { server, token, conversationId, agentId };
      reconnectAttempts = 0;
      monitorFailures = 0;
      manualVoiceEnd = false;
      status.textContent = 'Connectingâ€¦';
      try {
        await connectLivekit(storedVoiceSession, false);
      } catch (err) {
        console.error('[voice] connection failed', err);
        status.textContent = 'Connection failed. Please try again.';
        startBtn.classList.remove('hidden');
        endBtn.classList.add('hidden');
        const headerSpacerFail = document.getElementById('headerSpacer');
        if (headerSpacerFail) headerSpacerFail.classList.remove('hidden');
        const voiceWelcomeFail = document.getElementById('voiceWelcome');
        if (voiceWelcomeFail) voiceWelcomeFail.classList.remove('hidden');
      }
    }

    async function connectLivekit(sessionInfo, isReconnect){
      const status = document.getElementById('voiceStatus');
      const startBtn = document.getElementById('voiceStartBtn');
      const endBtn = document.getElementById('voiceEndBtn');
      const indicator = document.getElementById('agentSpeakingIndicator');

      if (!sessionInfo || !sessionInfo.server || !sessionInfo.token) {
        throw new Error('connectLivekit missing session info');
      }

      const { server, token } = sessionInfo;
      const { Room, RoomEvent, ParticipantKind } = window.LivekitClient || window.LiveKit || {};
      try { console.log('[voice] RoomEvent keys', RoomEvent ? Object.keys(RoomEvent) : null); } catch {}
      if (!Room) {
        status.textContent = 'Voice SDK not available.';
        throw new Error('LiveKit SDK missing');
      }

      const roomConn = new Room();
      __roomConn = roomConn;
      manualVoiceEnd = false;

      if (!window.__voicePlaybackPrimed) {
        try { await enableVoicePlayback(); } catch {}
      } else {
        hideVoicePlaybackPrompt();
      }

      try {
        roomConn.startAudio?.().catch((err) => {
          try { console.warn('[voice] startAudio deferred', err); } catch {}
          showVoicePlaybackPrompt();
        });
      } catch (err) {
        try { console.warn('[voice] startAudio threw', err); } catch {}
        showVoicePlaybackPrompt();
      }

      startBtn.classList.add('hidden');
      endBtn.classList.remove('hidden');
      const headerSpacer = document.getElementById('headerSpacer');
      if (headerSpacer) headerSpacer.classList.add('hidden');
      const voiceWelcome = document.getElementById('voiceWelcome');
      if (voiceWelcome) voiceWelcome.classList.add('hidden');

      try { roomConn.on(RoomEvent.ConnectionStateChanged, (state) => { try { console.log('[voice] connection state', state); } catch {} }); } catch {}
      try {
        roomConn.on(RoomEvent.AudioPlaybackStatusChanged, () => {
          try { console.log('[voice] AudioPlaybackStatusChanged canPlaybackAudio=', roomConn.canPlaybackAudio); } catch {}
          if (roomConn.canPlaybackAudio) {
            hideVoicePlaybackPrompt();
          } else {
            showVoicePlaybackPrompt();
          }
        });
      } catch {}
      try {
        if (RoomEvent?.RemoteTrackSubscribed) {
          roomConn.on(RoomEvent.RemoteTrackSubscribed, (track, pub, participant) => {
            try { console.log('[voice] RemoteTrackSubscribed event', { trackSid: track?.sid, pubSid: pub?.trackSid, participant: participant?.identity }); } catch {}
            handleRemoteAudioTrack(track, pub, participant);
          });
        }
        if (RoomEvent?.RemoteTrackUnsubscribed) {
          roomConn.on(RoomEvent.RemoteTrackUnsubscribed, (track, pub, participant) => {
            try { console.warn('[voice] RemoteTrackUnsubscribed event', { trackSid: track?.sid, pubSid: pub?.trackSid, participant: participant?.identity }); } catch {}
          });
        }
      } catch {}
      try {
        roomConn.on(RoomEvent.MediaDevicesError, (error) => {
          try { console.error('[voice] MediaDevicesError', error); } catch {}
          if (status) {
            status.textContent = 'Microphone error. Please check browser permissions and retry.';
          }
          showVoicePlaybackPrompt();
        });
      } catch {}
      try {
        roomConn.on(RoomEvent.TrackSubscriptionFailed, (sid, err) => {
          try {
            console.error('[voice] TrackSubscriptionFailed', {
              trackSid: sid,
              message: err?.message || String(err || ''),
              code: err?.code,
            });
          } catch {}
        });
      } catch {}
      try {
        roomConn.on(RoomEvent.TrackPublished, (pub, participant) => {
          try {
            console.log('[voice] TrackPublished', {
              kind: pub?.kind,
              trackSid: pub?.trackSid,
              muted: pub?.isMuted,
              participant: participant?.identity,
              subscribed: pub?.isSubscribed,
            });
          } catch {}

          if (String(pub?.kind || '').toLowerCase() === 'audio') {
            ensureAudioSubscribed(pub, 'TrackPublished');
            try {
              const track = resolvePublicationTrack(pub);
              if (track) { handleRemoteAudioTrack(track, pub, participant); }
            } catch {}
            try {
              if (pub?.on && !pub.__voice_track_published_hook) {
                pub.__voice_track_published_hook = true;
                pub.on('subscribed', (track) => {
                  try { console.log('[voice] publication subscribed callback', { trackSid: track?.sid, pubSid: pub?.trackSid }); } catch {}
                  handleRemoteAudioTrack(track, pub, participant);
                });
              }
            } catch {}
          }
        });
      } catch {}
      try {
        roomConn.on(RoomEvent.TrackUnpublished, (pub, participant) => {
          try {
            console.log('[voice] TrackUnpublished', {
              kind: pub?.kind,
              trackSid: pub?.trackSid,
              participant: participant?.identity,
            });
          } catch {}
        });
      } catch {}
      try {
        roomConn.on(RoomEvent.TrackMuted, (participant, pub) => {
          try {
            console.warn('[voice] TrackMuted', {
              participant: participant?.identity,
              trackSid: pub?.trackSid,
            });
          } catch {}
        });
      } catch {}
      try {
        roomConn.on(RoomEvent.TrackUnmuted, (participant, pub) => {
          try {
            console.log('[voice] TrackUnmuted', {
              participant: participant?.identity,
              trackSid: pub?.trackSid,
            });
          } catch {}
        });
      } catch {}
      try {
        roomConn.on(RoomEvent.TrackUnsubscribed, (track, pub, participant) => {
          try {
            console.warn('[voice] TrackUnsubscribed', {
              kind: track?.kind,
              trackSid: pub?.trackSid,
              participant: participant?.identity,
            });
          } catch {}

          if (track?.kind === 'audio') {
            try {
              const nodes = track.detach?.() || [];
              nodes.forEach((node) => { try { node.remove?.(); } catch {} });
            } catch {}
            try {
              const sidKey = pub?.trackSid || `audio-${track.sid || Date.now()}`;
              if (window.__lk_audio_elements && window.__lk_audio_elements[sidKey]) {
                try {
                  const el = window.__lk_audio_elements[sidKey];
                  if (el) {
                    el.pause?.();
                    el.srcObject = null;
                  }
                } catch {}
              }
            } catch {}

            if (!manualVoiceEnd) {
              ensureAudioSubscribed(pub, 'TrackUnsubscribed-resubscribe', 'warn');
              attachExistingRemoteAudio(roomConn);
            }
          }
        });
      } catch {}
      try {
        roomConn.on(RoomEvent.Disconnected, (reason) => {
          try { console.warn('[voice] Room disconnected', reason); } catch {}
          clearVoiceConnectionMonitor();
          if (manualVoiceEnd) {
            if (status) status.textContent = 'Call ended.';
          } else {
            if (status) {
              const reasonText = (reason && reason.reason) || reason?.toString?.() || 'unknown';
              status.textContent = `Connection lost (${reasonText}). Reconnectingâ€¦`;
            }
            tryVoiceReconnect('event-disconnected');
          }
        });
      } catch {}

      const connectTimeoutMs = 15000;
      try {
        const connectPromise = roomConn.connect(server, token, { autoSubscribe: true });
        await Promise.race([
          connectPromise,
          new Promise((_, rej) => setTimeout(() => rej(new Error('connect-timeout')), connectTimeoutMs))
        ]);
      } catch (err) {
        try { await roomConn.disconnect(); } catch {}
        throw err;
      }

      scheduleVoiceConnectionMonitor(roomConn);
      reconnectAttempts = 0;
      attachExistingRemoteAudio(roomConn);
      try {
        let tries = 0;
        const maxTries = 6;
        const interval = setInterval(() => {
          tries += 1;
          attachExistingRemoteAudio(roomConn);
          if (tries >= maxTries || !__roomConn || __roomConn !== roomConn) {
            clearInterval(interval);
          }
        }, 1000);
      } catch {}

      try {
        roomConn.startAudio?.().catch((err) => {
          try { console.warn('[voice] startAudio deferred', err); } catch {}
          showVoicePlaybackPrompt();
        });
      } catch (err) {
        try { console.warn('[voice] startAudio threw', err); } catch {}
        showVoicePlaybackPrompt();
      }

      try {
        // Enable microphone with echo cancellation and noise suppression to prevent
        // agent's TTS output from being picked up and transcribed as user speech
        const audioCaptureOptions = {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        };
        await roomConn.localParticipant.setMicrophoneEnabled(true, audioCaptureOptions, { preConnectBuffer: true });
        try { console.log('[voice] Microphone enabled with echo cancellation via setMicrophoneEnabled'); } catch {}
        if (status) {
          status.textContent = 'Microphone enabled. You can speak now.';
        }
      } catch (micErr) {
        try { console.error('[voice] setMicrophoneEnabled failed', micErr); } catch {}
        if (status) {
          status.textContent = 'Microphone unavailable. Check browser permissions and retry.';
        }
        showVoicePlaybackPrompt();
      }

      try {
        roomConn.on(RoomEvent.LocalTrackPublished, (pub) => {
          try { console.log('[voice] LocalTrackPublished', { kind: pub.kind, trackSid: pub.trackSid }); } catch {}
        });
      } catch {}

      try {
        roomConn.on(RoomEvent.ActiveSpeakersChanged, (speakers) => {
          try { console.log('[voice] ActiveSpeakers', speakers?.map?.(s => ({ id: s.identity, lvl: s.audioLevel })) || speakers); } catch {}
        });
      } catch {}
      try {
        if (RoomEvent?.RemoteTrackPublished) {
          roomConn.on(RoomEvent.RemoteTrackPublished, (pub, participant) => {
            try {
              console.log('[voice] RemoteTrackPublished', {
                kind: pub?.kind,
                trackSid: pub?.trackSid,
                participant: participant?.identity,
                subscribed: pub?.isSubscribed,
              });
            } catch {}

            if (String(pub?.kind || '').toLowerCase() === 'audio') {
              ensureAudioSubscribed(pub, 'RemoteTrackPublished');
              try {
                const maybeTrack = resolvePublicationTrack(pub);
                if (maybeTrack) { handleRemoteAudioTrack(maybeTrack, pub, participant); }
              } catch {}
              try {
                if (pub?.on && !pub.__voice_remote_published_hook) {
                  pub.__voice_remote_published_hook = true;
                  pub.on('subscribed', (track) => {
                    try { console.log('[voice] pub subscribed event', { trackSid: track?.sid, pubSid: pub?.trackSid }); } catch {}
                    handleRemoteAudioTrack(track, pub, participant);
                  });
                  pub.on('unsubscribed', () => {
                    try {
                      const sidKey = pub?.trackSid;
                      if (sidKey && window.__lk_audio_elements?.[sidKey]) {
                        const el = window.__lk_audio_elements[sidKey];
                        el?.pause?.();
                        el.srcObject = null;
                      }
                    } catch {}
                  });
                }
              } catch {}
            }
          });
        }
      } catch {}
      try {
        roomConn.on(RoomEvent.DataReceived, (payload, participant, topic) => {
          try {
            const decoder = window.__lk_decoder || new TextDecoder();
            window.__lk_decoder = decoder;
            let textPayload = '';
            if (typeof payload === 'string') {
              textPayload = payload;
            } else if (payload instanceof ArrayBuffer) {
              textPayload = decoder.decode(payload);
            } else if (ArrayBuffer.isView(payload)) {
              textPayload = decoder.decode(payload.buffer);
            }
            if (!textPayload) { return; }
            let parsed = null;
            try { parsed = JSON.parse(textPayload); } catch {}
            if (parsed && (parsed.content || parsed.transcript || parsed.text)) {
              parsed.id = parsed.id || `${topic || 'data'}-${Date.now()}`;
              const pIdentity = String(participant?.identity || '').toLowerCase();
              const fallbackRole = parsed.role || (pIdentity.includes('agent') ? 'assistant' : 'user');
              ingestLivekitTranscript(parsed, fallbackRole);
            }
          } catch (err) {
            try { console.warn('[voice] failed to process LiveKit data payload', err); } catch {}
          }
        });
      } catch {}
      try {
        const transcriptionEvent = RoomEvent.TranscriptionReceived;
        if (transcriptionEvent) {
          roomConn.on(transcriptionEvent, (segments, participant, publication) => {
            try {
              console.log('[voice] TranscriptionReceived', {
                segments: segments?.length,
                participant: participant?.identity,
                isFinal: segments?.[0]?.final
              });
              // LiveKit sends segments array - combine them into transcript
              if (segments && segments.length > 0) {
                const text = segments.map(s => s.text || '').join(' ').trim();
                const isFinal = segments.some(s => s.final);
                const segmentId = segments[0]?.id || `lk-${Date.now()}`;
                if (text) {
                  // Determine role from participant identity
                  const isAgent = participant && (
                    String(participant.identity || '').toLowerCase().includes('agent') ||
                    participant.kind === (ParticipantKind?.AGENT || 1)
                  );
                  const role = isAgent ? 'assistant' : 'user';

                  if (isFinal) {
                    // Final transcript - display it
                    ingestLivekitTranscript({
                      text,
                      role,
                      id: segmentId,
                      createdAt: new Date().toISOString()
                    }, role);
                  } else {
                    // Interim transcript - update existing bubble or create temp one
                    const tempId = `interim-${participant?.identity || 'unknown'}`;
                    if (!updateVoiceTranscript(tempId, text, role)) {
                      appendVoiceTranscript(role, text, new Date().toISOString(), [], tempId);
                    }
                  }
                }
              }
            } catch (err) {
              console.warn('[voice] TranscriptionReceived handler error', err);
            }
          });
        }
      } catch {}

      roomConn.on(RoomEvent.TrackSubscribed, (track, pub, participant) => {
        try {
          console.log('[voice] TrackSubscribed', {
            kind: track?.kind,
            trackSid: pub?.trackSid,
            participant: participant?.identity,
            participantKind: participant?.kind,
            muted: pub?.isMuted,
          });
        } catch {}

        if (track.kind === 'audio') {
          handleRemoteAudioTrack(track, pub, participant);
        }
      });

      roomConn.on(RoomEvent.ParticipantConnected, (p) => {
        try {
          const agentKind = (ParticipantKind && ParticipantKind.AGENT) || 'agent';
          if (p && (p.kind === agentKind || String(p.identity||'').toLowerCase().includes('agent'))) {
            status.textContent = 'Agent connected. You can speak now.';
          } else {
            status.textContent = 'Connected. You can speak now.';
          }
          attachExistingRemoteAudio(roomConn);
        } catch {}
      });

      try {
        roomConn.on(RoomEvent.ActiveSpeakersChanged, (speakers) => {
          try {
            const anyAgentSpeaking = (speakers||[]).some(s => String(s.identity||'').toLowerCase().includes('agent') && (s.audioLevel||0) > 0);
            if (indicator){ indicator.classList.toggle('hidden', !anyAgentSpeaking); }
            if (anyAgentSpeaking) {
              attachExistingRemoteAudio(roomConn);
            }
          } catch {}
        });
      } catch {}

      roomConn.on(RoomEvent.ParticipantDisconnected, (participant) => {
        try { console.log('[voice] Participant disconnected', participant?.identity); } catch {}
        if (!manualVoiceEnd && participant && String(participant.identity || '').toLowerCase().includes('agent')) {
          setTimeout(() => tryVoiceReconnect('agent-left'), 1200);
        }
      });

      setTimeout(() => {
        try {
          const participants = Array.from(roomConn.remoteParticipants.values());
          const hasAgent = participants.some((rp) => String(rp.identity||'').toLowerCase().includes('agent'));
          const hasRemoteAudio = participants.some((rp) => participantHasAudioTrack(rp));
          if (!hasAgent) { status.textContent = 'Waiting for agent to joinâ€¦'; }
          else if (!hasRemoteAudio) { status.textContent = 'Agent joined but no audio yetâ€¦'; }
        } catch {}
      }, 8000);

      status.textContent = isReconnect ? 'Reconnected. You can speak now.' : 'Connected. You can speak now.';
    }

    async function endVoice(){
      const status = document.getElementById('voiceStatus');
      const startBtn = document.getElementById('voiceStartBtn');
      const endBtn = document.getElementById('voiceEndBtn');
      manualVoiceEnd = true;
      hideVoicePlaybackPrompt();
      clearVoiceConnectionMonitor();
      monitorFailures = 0;
      reconnectAttempts = 0;
      try {
        stopVoiceTranscriptStream();
        if (__roomConn) {
          try {
            const locals = [];
            iterateTrackPublications(__roomConn.localParticipant, (pub) => { locals.push(pub); });
            for (const pub of locals) { try { await pub.unpublish?.(); } catch {} }
          } catch {}
          await __roomConn.disconnect();
        }
      } catch {}
      __roomConn = null;
      storedVoiceSession = null;
      status.textContent = 'Call ended.';
      endBtn.classList.add('hidden');
      startBtn.classList.remove('hidden');
      const headerSpacerEnd = document.getElementById('headerSpacer');
      if (headerSpacerEnd) headerSpacerEnd.classList.remove('hidden');
      const voiceWelcomeEnd = document.getElementById('voiceWelcome');
      if (voiceWelcomeEnd) voiceWelcomeEnd.classList.remove('hidden');
      setVoiceAudioMuted(true);
      try {
        const elementsMap = window.__lk_audio_elements || {};
        Object.entries(elementsMap).forEach(([key, el]) => {
          try { el?.remove?.(); } catch {}
          delete elementsMap[key];
        });
      } catch {}
      try { window.__lk_transcript_seen = {}; __seenTranscriptIds = new Set(); __recentTranscriptTexts = []; } catch {}
      try { delete window.__voicePlaybackPrimed; } catch {}
    }

    // =====================================================
    // VIDEO CHAT FUNCTIONS
    // =====================================================
    let __videoRoomConn = null;
    let storedVideoSession = null;
    let manualVideoEnd = false;

    // Storage for video mode audio elements (similar to voice mode)
    window.__video_audio_elements = window.__video_audio_elements || {};

    // Handle remote audio track from agent in video mode (TTS output)
    function handleVideoRemoteAudioTrack(track, pub, participant) {
      if (!track || String(track.kind || '').toLowerCase() !== 'audio') { return; }
      try { console.log('[video] handleVideoRemoteAudioTrack', { sid: track.sid, pubSid: pub?.trackSid, participant: participant?.identity }); } catch {}

      try {
        track.setEnabled?.(true);
        track.setVolume?.(1.0);
      } catch {}

      const sidKey = pub?.trackSid || `video-audio-${track.sid || Date.now()}`;
      try { console.log('[video] audio sid key', sidKey); } catch {}

      // Detach from any previous elements
      try {
        const previous = track.detach?.() || [];
        previous.forEach((node) => { try { node.remove?.(); } catch {} });
      } catch {}

      let el = window.__video_audio_elements[sidKey];
      try { console.log('[video] existing audio element?', !!el); } catch {}
      const needAttach = !el || !document.body.contains(el);
      if (needAttach) {
        try { el?.pause?.(); el?.remove?.(); } catch {}
        el = document.createElement('audio');
        el.autoplay = true;
        el.playsInline = true;
        el.muted = false; // Video mode audio should not be muted
        try { el.volume = 1.0; } catch {}
        el.style.position = 'fixed';
        el.style.bottom = '8px';
        el.style.right = '8px';
        el.style.width = '0px';
        el.style.height = '0px';
        el.style.opacity = '0';
        el.style.pointerEvents = 'none';
        document.body.appendChild(el);
        window.__video_audio_elements[sidKey] = el;
        try { console.log('[video] created new audio element', sidKey); } catch {}
      }

      try { track.attach(el); } catch {}

      try {
        console.log('[video] attempting el.play');
        console.log('[video] audio element state BEFORE play:', {
          muted: el.muted,
          volume: el.volume,
          paused: el.paused,
          readyState: el.readyState,
          srcObject: !!el.srcObject,
          currentSrc: el.currentSrc?.slice(0, 50) || 'none',
        });
        const playPromise = el.play?.();
        if (playPromise && typeof playPromise.then === 'function') {
          playPromise
            .then(() => {
              console.log('[video] el.play resolved - agent audio playing');
              console.log('[video] audio element state AFTER play:', {
                muted: el.muted,
                volume: el.volume,
                paused: el.paused,
                readyState: el.readyState,
                srcObject: !!el.srcObject,
                currentTime: el.currentTime,
              });
            })
            .catch((err) => {
              console.warn('[video] audio play blocked', err);
            });
        }
      } catch (err) {
        try { console.warn('[video] audio play failed', err); } catch {}
      }

      try { document.getElementById('videoStatus').textContent = 'Video & audio connected'; } catch {}
    }

    // Cleanup video audio elements
    function cleanupVideoAudioElements() {
      try {
        for (const key in window.__video_audio_elements) {
          const el = window.__video_audio_elements[key];
          try { el?.pause?.(); el?.remove?.(); } catch {}
        }
        window.__video_audio_elements = {};
      } catch {}
    }

    // Video model loading progress functions
    function updateVideoLoadingProgress(progress, message) {
      try {
        const progressBar = document.getElementById('videoLoaderProgress');
        const percentText = document.getElementById('videoLoaderPercent');
        const loaderText = document.getElementById('videoLoaderText');

        if (progressBar) {
          progressBar.style.width = `${Math.min(100, Math.max(0, progress))}%`;
        }
        if (percentText) {
          percentText.textContent = `${Math.round(progress)}%`;
        }
        if (loaderText && message) {
          loaderText.textContent = message;
        }
        console.log('[video] Model loading:', progress + '%', message);
      } catch (err) {
        console.warn('[video] Failed to update loading progress', err);
      }
    }

    function showVideoModelLoader() {
      try {
        const loader = document.getElementById('videoModelLoader');
        if (loader) {
          loader.classList.remove('hidden');
          loader.style.display = 'flex';
          // Reset progress
          updateVideoLoadingProgress(5, 'Initializing avatar...');
        }
      } catch {}
    }

    function hideVideoModelLoader() {
      try {
        const loader = document.getElementById('videoModelLoader');
        if (loader) {
          // Smooth fade out
          loader.style.transition = 'opacity 0.5s ease-out';
          loader.style.opacity = '0';
          setTimeout(() => {
            loader.classList.add('hidden');
            loader.style.display = 'none';
            loader.style.opacity = '1';
          }, 500);
        }
        console.log('[video] Model ready - hiding loader');
      } catch {}
    }

    async function startVideo() {
      try { console.log('[video] startVideo invoked'); } catch {}
      const status = document.getElementById('videoStatus');
      const startBtn = document.getElementById('videoStartBtn');
      const welcome = document.getElementById('videoWelcome');
      const container = document.getElementById('videoContainer');
      
      status.textContent = 'Connecting to video...';
      
      try { console.log('[video] ensuring session'); } catch {}
      const {ok, session, manualToken} = await ensureSession();
      
      const hasSupabaseUser = !!(session?.user?.id);
      const hasManualToken = !!manualToken;
      const injectedUserId = window.__client_user_id;
      
      if (!hasSupabaseUser && !hasManualToken && !injectedUserId) {
        status.textContent = 'Please sign in to start video.';
        return;
      }
      
      const bearer = manualToken || session?.access_token || '';
      const room = `video_${AGENT_SLUG}_${Date.now()}`;
      const payload = {
        agent_slug: AGENT_SLUG,
        client_id: CLIENT_ID,
        mode: 'video',
        room_name: room,
        platform: 'livekit',
        user_id: injectedUserId || session?.user?.id || 'preview-user',
      };

      try { console.log('[video] trigger payload prepared', payload); } catch {}
      const res = await fetch('/api/v1/trigger-agent', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json', 'Authorization': `Bearer ${bearer}` },
        body: JSON.stringify(payload),
      });

      if (!res.ok) {
        const errText = await res.text().catch(() => '');
        console.error('[video] trigger failed', res.status, errText);
        status.textContent = 'Video start failed: ' + (errText || res.status);
        return;
      }

      const data = await res.json();
      try { console.log('[video] trigger response', data); } catch {}

      if (!data || !data.data) {
        status.textContent = 'Error: trigger-agent returned no data.';
        return;
      }

      const info = data.data.livekit_config;
      const server = info?.server_url;
      const token = info?.user_token;
      const conversationId = data.data.conversation_id;
      const agentId = data.data.agent_context?.agent_id || '';
      
      if (!server || !token) { 
        status.textContent = 'Video start response incomplete.'; 
        return; 
      }
      
      storedVideoSession = { server, token, conversationId, agentId };
      manualVideoEnd = false;
      
      try {
        await connectVideoLivekit(storedVideoSession);
        // Start transcript stream for video mode
        if (conversationId) {
          startVideoTranscriptStream(conversationId, CLIENT_ID, agentId);
        }
      } catch (err) {
        console.error('[video] connection failed', err);
        status.textContent = 'Video connection failed: ' + (err.message || err);
        welcome.classList.remove('hidden');
        container.classList.add('hidden');
      }
    }

    async function connectVideoLivekit(sessionData) {
      const { server, token } = sessionData;
      const status = document.getElementById('videoStatus');
      const welcome = document.getElementById('videoWelcome');
      const container = document.getElementById('videoContainer');
      const videoElement = document.getElementById('remoteVideoElement');
      
      const LiveKit = window.LivekitClient || window.LiveKit || {};
      const { Room, RoomEvent, VideoPresets, ConnectionState } = LiveKit;

      if (!Room || !RoomEvent) {
        status.textContent = 'Video SDK not available.';
        throw new Error('LiveKit SDK not loaded');
      }

      __videoRoomConn = new Room({
        adaptiveStream: true,
        dynacast: true,
        videoCaptureDefaults: { resolution: VideoPresets?.h720?.resolution },
      });
      
      // Hide welcome, show video container with loading overlay
      welcome.classList.add('hidden');
      container.classList.remove('hidden');
      showVideoModelLoader();  // Show loading overlay while model loads
      status.textContent = 'Connecting...';
      
      // Handle video track subscription
      __videoRoomConn.on(RoomEvent.TrackSubscribed, (track, pub, participant) => {
        try {
          console.log('[video] TrackSubscribed', {
            kind: track?.kind,
            kindType: typeof track?.kind,
            trackSid: track?.sid,
            participant: participant?.identity,
            isVideoTrack: String(track?.kind || '').toLowerCase() === 'video',
            isAudioTrack: String(track?.kind || '').toLowerCase() === 'audio'
          });
        } catch {}
        // Use string conversion for robust comparison (SDK may return string or enum)
        const trackKind = String(track?.kind || '').toLowerCase();
        if (trackKind === 'video') {
          try {
            track.attach(videoElement);
            status.textContent = 'Video connected';
            console.log('[video] Video track attached to element');
            // Hide loader as fallback when video track arrives (model_ready event is preferred)
            hideVideoModelLoader();
            // Log video dimensions after a short delay to let the stream start
            setTimeout(() => {
              console.log('[video] Video track dimensions:', {
                videoWidth: videoElement.videoWidth,
                videoHeight: videoElement.videoHeight,
                aspectRatio: videoElement.videoWidth && videoElement.videoHeight
                  ? (videoElement.videoWidth / videoElement.videoHeight).toFixed(2)
                  : 'unknown',
                trackDimensions: track.dimensions,
              });
            }, 1500);
          } catch (attachErr) {
            console.error('[video] Failed to attach video track', attachErr);
          }
        } else if (trackKind === 'audio') {
          // Handle audio track from the agent (TTS output)
          try {
            console.log('[video] Audio track received from agent');
            handleVideoRemoteAudioTrack(track, pub, participant);
          } catch (audioErr) {
            console.error('[video] Failed to handle audio track', audioErr);
          }
        }
      });

      __videoRoomConn.on(RoomEvent.TrackUnsubscribed, (track) => {
        try { console.log('[video] TrackUnsubscribed', track?.kind); } catch {}
        const trackKind = String(track?.kind || '').toLowerCase();
        if (trackKind === 'video') {
          try { track.detach(videoElement); } catch {}
        } else if (trackKind === 'audio') {
          // Detach audio track
          try {
            track.detach?.();
            console.log('[video] Audio track detached');
          } catch {}
        }
      });
      
      __videoRoomConn.on(RoomEvent.ConnectionStateChanged, (state) => {
        try { console.log('[video] ConnectionState', state); } catch {}
        if (state === ConnectionState.Disconnected && !manualVideoEnd) {
          status.textContent = 'Video disconnected';
        }
      });
      
      __videoRoomConn.on(RoomEvent.DataReceived, (payload, participant) => {
        try {
          const text = new TextDecoder().decode(payload);
          const data = JSON.parse(text);
          if (data.type === 'transcript') {
            appendVideoTranscript(data.role, data.text);
          } else if (data.type === 'model_loading') {
            // Handle model loading progress updates
            updateVideoLoadingProgress(data.progress, data.message);
          } else if (data.type === 'model_ready') {
            // Model is ready - hide the loader and show video
            hideVideoModelLoader();
          }
        } catch {}
      });

      // Handle when participants connect (avatar agent joining)
      __videoRoomConn.on(RoomEvent.ParticipantConnected, (participant) => {
        try {
          console.log('[video] ParticipantConnected:', participant?.identity);
          // When participant connects, check for video and audio tracks
          if (participant?.trackPublications) {
            participant.trackPublications.forEach((pub) => {
              const trackKind = String(pub?.kind || pub?.track?.kind || '').toLowerCase();
              console.log('[video] Participant track:', { kind: trackKind, subscribed: pub?.isSubscribed });
              if (trackKind === 'video' && pub?.track) {
                try {
                  pub.track.attach(videoElement);
                  status.textContent = 'Video connected';
                  console.log('[video] Attached video from new participant');
                } catch (attachErr) {
                  console.error('[video] Failed to attach video', attachErr);
                }
              } else if (trackKind === 'audio' && pub?.track) {
                try {
                  handleVideoRemoteAudioTrack(pub.track, pub, participant);
                  console.log('[video] Attached audio from new participant');
                } catch (attachErr) {
                  console.error('[video] Failed to attach audio', attachErr);
                }
              }
            });
          }
        } catch {}
      });

      // Handle when tracks are published by remote participants
      __videoRoomConn.on(RoomEvent.TrackPublished, (pub, participant) => {
        try {
          const trackKind = String(pub?.kind || '').toLowerCase();
          console.log('[video] TrackPublished:', { kind: trackKind, participant: participant?.identity, trackSid: pub?.trackSid });
        } catch {}
      });

      await __videoRoomConn.connect(server, token);
      try { console.log('[video] connected to room'); } catch {}

      // Enable audio playback
      try {
        await __videoRoomConn.startAudio();
      } catch (err) {
        try { console.warn('[video] startAudio failed', err); } catch {}
      }

      // Enable microphone for user voice capture (STT)
      try {
        const audioCaptureOptions = {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        };
        await __videoRoomConn.localParticipant.setMicrophoneEnabled(true, audioCaptureOptions, { preConnectBuffer: true });
        console.log('[video] Microphone enabled with echo cancellation');
        status.textContent = 'Video & audio connected. Microphone enabled.';
      } catch (micErr) {
        console.error('[video] Failed to enable microphone', micErr);
        status.textContent = 'Video connected but microphone unavailable. Check browser permissions.';
      }

      // Check for already-published video and audio tracks from remote participants
      try {
        const remoteParticipants = __videoRoomConn.remoteParticipants || __videoRoomConn.participants;
        if (remoteParticipants) {
          console.log('[video] Checking existing participants:', remoteParticipants.size || Object.keys(remoteParticipants).length);
          remoteParticipants.forEach((participant, identity) => {
            console.log('[video] Checking participant:', identity, participant?.trackPublications?.size);
            if (participant?.trackPublications) {
              participant.trackPublications.forEach((pub) => {
                const trackKind = String(pub?.kind || pub?.track?.kind || '').toLowerCase();
                console.log('[video] Existing track:', { kind: trackKind, subscribed: pub?.isSubscribed, trackSid: pub?.trackSid });
                if (trackKind === 'video' && pub?.track) {
                  try {
                    pub.track.attach(videoElement);
                    status.textContent = 'Video connected';
                    console.log('[video] Attached existing video track');
                  } catch (attachErr) {
                    console.error('[video] Failed to attach existing video track', attachErr);
                  }
                } else if (trackKind === 'audio' && pub?.track) {
                  try {
                    handleVideoRemoteAudioTrack(pub.track, pub, participant);
                    console.log('[video] Attached existing audio track');
                  } catch (attachErr) {
                    console.error('[video] Failed to attach existing audio track', attachErr);
                  }
                }
              });
            }
          });
        }
      } catch (existingErr) {
        console.error('[video] Error checking existing tracks', existingErr);
      }
    }

    async function endVideo() {
      const status = document.getElementById('videoStatus');
      const welcome = document.getElementById('videoWelcome');
      const container = document.getElementById('videoContainer');
      const videoElement = document.getElementById('remoteVideoElement');

      manualVideoEnd = true;
      stopVideoTranscriptStream();

      try {
        if (__videoRoomConn) {
          await __videoRoomConn.disconnect();
        }
      } catch {}

      __videoRoomConn = null;
      storedVideoSession = null;

      // Detach video
      videoElement.srcObject = null;

      // Cleanup audio elements
      cleanupVideoAudioElements();

      // Reset UI
      container.classList.add('hidden');
      welcome.classList.remove('hidden');
      status.textContent = 'Video ended.';

      // Clear transcripts and deduplication state
      const transcriptList = document.getElementById('videoTranscriptList');
      if (transcriptList) transcriptList.innerHTML = '';
      __videoTranscriptIds.clear();
      __videoSeenTranscriptIds.clear();
    }

    function toggleVideoFullscreen() {
      const container = document.getElementById('videoContainer');
      if (!document.fullscreenElement) {
        container.requestFullscreen?.() || container.webkitRequestFullscreen?.();
      } else {
        document.exitFullscreen?.() || document.webkitExitFullscreen?.();
      }
    }

    // Video transcript handling
    let __videoTranscriptChannel = null;
    let __videoTranscriptIds = new Map(); // Map of id -> element for updates
    let __videoSeenTranscriptIds = new Set(); // Deduplication set (mirrors voice mode)

    // Check if video transcript already seen (deduplication - mirrors voice mode logic)
    function markVideoTranscriptSeen(id) {
      if (!id) return false;
      const key = String(id);
      if (__videoSeenTranscriptIds.has(key)) return true;
      __videoSeenTranscriptIds.add(key);
      return false;
    }

    function appendVideoTranscript(role, text, id) {
      const list = document.getElementById('videoTranscriptList');
      if (!list || !text) return;

      // Deduplication check - skip if already seen (mirrors voice mode)
      if (id && markVideoTranscriptSeen(id)) {
        console.log('[video] appendVideoTranscript SKIPPED (already seen):', id?.slice(0, 8), role);
        return;
      }

      const isUser = role === 'user';

      // Check if this transcript already exists in DOM (for updates via INSERT after UPDATE race)
      if (id && __videoTranscriptIds.has(id)) {
        const existingBubble = __videoTranscriptIds.get(id);
        if (existingBubble) {
          // Find the text span inside the bubble and update if new text is longer (for user) or always (for assistant)
          const textSpan = existingBubble.querySelector('.transcript-text');
          if (textSpan) {
            if (isUser) {
              // For user: only update if new text is longer (handles out-of-order STT chunks)
              if (text.length >= (textSpan.textContent || '').length) {
                textSpan.textContent = text;
              }
            } else {
              textSpan.textContent = text;
            }
          }
          return;
        }
      }

      console.log('[video] appendVideoTranscript ADDING:', id?.slice(0, 8), role, text?.slice(0, 30));

      // Create wrapper for alignment
      const wrapper = document.createElement('div');
      wrapper.className = isUser ? 'flex justify-end' : 'flex justify-start';
      if (id) wrapper.setAttribute('data-video-transcript-id', id);

      // Create styled bubble
      const bubble = document.createElement('div');
      bubble.className = isUser
        ? 'video-transcript-bubble video-transcript-user max-w-[75%] px-3 py-1.5 rounded-xl text-white text-sm'
        : 'video-transcript-bubble video-transcript-assistant max-w-[75%] px-3 py-1.5 rounded-xl text-white/90 text-sm';

      // Add role indicator for assistant
      if (!isUser) {
        const roleLabel = document.createElement('span');
        roleLabel.className = 'text-brand-teal text-xs font-medium mr-1.5';
        roleLabel.textContent = 'â—';
        bubble.appendChild(roleLabel);
      }

      // Add text content (linkify URLs for user messages)
      const textSpan = document.createElement('span');
      textSpan.className = 'transcript-text';
      if (isUser) {
        textSpan.innerHTML = escapeAndLinkify(text);
      } else {
        textSpan.textContent = text;
      }
      bubble.appendChild(textSpan);

      if (id) {
        bubble.dataset.transcriptId = id;
        __videoTranscriptIds.set(id, bubble);
      }

      wrapper.appendChild(bubble);
      list.appendChild(wrapper);

      // Scroll to bottom
      list.scrollTop = list.scrollHeight;

      // Fade out older transcripts for visual hierarchy
      const wrappers = list.querySelectorAll(':scope > div');
      wrappers.forEach((w, i) => {
        const bubbleEl = w.querySelector('.video-transcript-bubble');
        if (bubbleEl && i < wrappers.length - 3) {
          bubbleEl.classList.add('video-transcript-fade');
        } else if (bubbleEl) {
          bubbleEl.classList.remove('video-transcript-fade');
        }
      });

      // Keep only last 6 transcript entries for compact overlay
      while (list.children.length > 6) {
        const removed = list.firstChild;
        const removedBubble = removed?.querySelector?.('[data-transcript-id]');
        if (removedBubble && removedBubble.dataset.transcriptId) {
          __videoTranscriptIds.delete(removedBubble.dataset.transcriptId);
          // Note: don't remove from __videoSeenTranscriptIds - still want dedup
        }
        list.removeChild(removed);
      }
    }

    function updateVideoTranscript(id, text, role) {
      if (!id) return;
      const existingBubble = __videoTranscriptIds.get(id);
      if (existingBubble) {
        const textSpan = existingBubble.querySelector('.transcript-text');
        if (textSpan) {
          const isUser = role === 'user';
          if (isUser) {
            // For user messages: only update if new text is longer
            // This prevents out-of-order STT updates from overwriting the full message
            const currentText = textSpan.textContent || '';
            if (text.length >= currentText.length) {
              textSpan.innerHTML = escapeAndLinkify(text);
            }
            // else: skip update - probably an out-of-order interim result
          } else {
            // For assistant messages: always update
            textSpan.textContent = text;
          }
        }
      }
    }

    async function startVideoTranscriptStream(conversationId, clientId, agentId) {
      // Use the same transcript Supabase client as voice mode
      const sbClient = getTranscriptSupabase();
      if (!conversationId || !sbClient) {
        console.warn('[video] Cannot start transcript stream - missing conversationId or Supabase client');
        return;
      }

      // Ensure authentication before subscribing
      const authorized = await ensureTranscriptSupabaseAuth();
      if (!authorized) {
        console.warn('[video] Cannot start transcript stream - auth failed');
        return;
      }

      stopVideoTranscriptStream();
      // Clear deduplication state for new conversation (mirrors voice mode)
      __videoTranscriptIds.clear();
      __videoSeenTranscriptIds.clear();

      console.log('[video] Starting transcript stream for conversation:', conversationId?.slice(0, 8));

      __videoTranscriptChannel = sbClient
        .channel(`video-transcripts:${conversationId}`)
        .on('postgres_changes',
          { event: 'INSERT', schema: 'public', table: 'conversation_transcripts', filter: `conversation_id=eq.${conversationId}` },
          (payload) => {
            const row = payload.new;
            if (row && row.content) {
              console.log('[video] INSERT transcript:', row.id?.slice(0, 8), row.role, 'seen:', __videoSeenTranscriptIds.has(row.id));
              appendVideoTranscript(row.role, row.content, row.id);
            }
          }
        )
        .on('postgres_changes',
          { event: 'UPDATE', schema: 'public', table: 'conversation_transcripts', filter: `conversation_id=eq.${conversationId}` },
          (payload) => {
            const row = payload.new;
            if (row && row.content && row.id) {
              console.log('[video] UPDATE transcript:', row.id?.slice(0, 8), row.role);
              // Pass role for the length-check logic on user messages
              updateVideoTranscript(row.id, row.content, row.role);
            }
          }
        )
        .subscribe((status) => {
          console.log('[video] Transcript channel status:', status);
        });
    }

    function stopVideoTranscriptStream() {
      if (__videoTranscriptChannel) {
        try { __videoTranscriptChannel.unsubscribe(); } catch {}
        __videoTranscriptChannel = null;
      }
      __videoTranscriptIds.clear();
      // Don't clear __videoSeenTranscriptIds here - keep dedup across reconnects
    }

    // Listen for admin preview postMessage with Supabase tokens
    window.addEventListener('message', async (evt) => {
      try {
        const data = evt && evt.data ? evt.data : null;
        if (!data || data.type !== 'supabase-session') return;
        if (data.access_token) {
          manualToken = data.access_token;
          if (data.client_user_id) { window.__client_user_id = data.client_user_id; }
          // If refresh_token is provided, set full Supabase session for a seamless experience
          if (data.refresh_token && sb && sb.auth && sb.auth.setSession) {
            try {
              await sb.auth.setSession({ access_token: data.access_token, refresh_token: data.refresh_token });
              const { data: s } = await sb.auth.getSession();
              session = (s && s.session) ? s.session : null;
            } catch (e) { /* fallback to manual token only */ }
          }
          if (data.client_supabase_access_token) {
            CLIENT_SUPABASE_BOOTSTRAP.access_token = data.client_supabase_access_token;
            CLIENT_SUPABASE_BOOTSTRAP.refresh_token = data.client_supabase_refresh_token;
            const clientSb = getTranscriptSupabase();
            if (clientSb) {
              try {
                await clientSb.auth.setSession({
                  access_token: CLIENT_SUPABASE_BOOTSTRAP.access_token,
                  refresh_token: CLIENT_SUPABASE_BOOTSTRAP.refresh_token || null
                });
                if (pendingTranscriptStart) {
                  const pending = pendingTranscriptStart;
                  pendingTranscriptStart = null;
                  setTimeout(() => {
                    startVoiceTranscriptStream(pending.conversationId, pending.clientId, pending.agentId);
                  }, 250);
                } else if (storedVoiceSession && storedVoiceSession.conversationId) {
                  startVoiceTranscriptStream(storedVoiceSession.conversationId, CLIENT_ID, storedVoiceSession.agentId);
                }
              } catch (err) {
                console.warn('[voice] failed to apply client Supabase session from message', err);
              }
            }
          }
          setAuthVisible(false); setChatVisible(true);
          try { document.getElementById('voiceStartBtn').disabled = false; } catch {}
        }
      } catch {}
    });

    // If loaded inside admin modal, make subtle style tweaks
    (function(){
      if (SOURCE === 'admin') {
        try { document.body.style.background = 'transparent'; } catch {}
      }
    })();

    // WordPress Bridge: Request session token from parent window if in iframe
    // Returns a promise that resolves when bridge auth completes (or times out)
    function initWordPressBridge() {
      return new Promise((resolve) => {
        if (window.self === window.top) {
          resolve(false); // Not in iframe, no bridge needed
          return;
        }

        try {
          console.log('[wordpress-bridge] Embed loaded in iframe, requesting token from parent');

          // Set a timeout in case parent doesn't respond (e.g., not a WordPress site)
          const timeout = setTimeout(() => {
            console.log('[wordpress-bridge] No response from parent, proceeding without bridge auth');
            resolve(false);
          }, 3000);

          // Listen for token response from parent
          window.addEventListener('message', async (event) => {
            if (!event.data || (event.data.type !== 'SKF_TOKEN' && event.data.type !== 'SKF_TOKEN_ERROR')) {
              return;
            }

            clearTimeout(timeout);

            if (event.data.type === 'SKF_TOKEN_ERROR') {
              console.error('[wordpress-bridge] Token error from parent:', event.data.error);
              resolve(false);
              return;
            }

            try {
              console.log('[wordpress-bridge] Received token from parent', event.origin);

              if (event.data.debug) {
                console.log('[wordpress-bridge] Debug data:', event.data.debug);
              }

              if (event.data.session) {
                const sessionData = event.data.session;
                console.log('[wordpress-bridge] Setting Supabase session');

                // The WordPress session returns tokens from the CLIENT Supabase project
                // We need to use the supabase_url and supabase_anon_key from the response
                // to create a client that can verify these tokens
                let authClient = sb;
                let dynamicClient = null;

                if (sessionData.supabase_url && sessionData.supabase_anon_key) {
                  // WordPress session includes client Supabase credentials - use them
                  console.log('[wordpress-bridge] Using client Supabase from session response:', sessionData.supabase_url);
                  dynamicClient = supabase.createClient(sessionData.supabase_url, sessionData.supabase_anon_key);
                  authClient = dynamicClient;
                }

                // Set the session in the appropriate Supabase client
                const { data, error } = await authClient.auth.setSession({
                  access_token: sessionData.access_token,
                  refresh_token: sessionData.refresh_token
                });

                if (error) {
                  console.error('[wordpress-bridge] Failed to set session:', error);
                  resolve(false);
                } else {
                  console.log('[wordpress-bridge] Session established successfully');

                  // Store the token for API calls
                  manualToken = sessionData.access_token;

                  // If we used a dynamic client, also update the client Supabase bootstrap
                  if (dynamicClient) {
                    // This token is for the CLIENT Supabase, set it up for transcripts
                    CLIENT_SUPABASE_BOOTSTRAP.access_token = sessionData.access_token;
                    CLIENT_SUPABASE_BOOTSTRAP.refresh_token = sessionData.refresh_token;
                    // Store the dynamic client for transcript use
                    transcriptSupabase = dynamicClient;
                    // Also set the session variable so getEffectiveUserId() works
                    session = {
                      access_token: sessionData.access_token,
                      refresh_token: sessionData.refresh_token,
                      user: sessionData.user
                    };
                  } else {
                    session = data.session;
                    // Also set client Supabase session if available
                    if (CLIENT_SUPABASE_BOOTSTRAP.access_token) {
                      await ensureTranscriptSupabaseAuth();
                    }
                  }
                  resolve(true);
                }
              } else {
                resolve(false);
              }
            } catch (err) {
              console.error('[wordpress-bridge] Error processing token:', err);
              resolve(false);
            }
          });

          // Request token from parent, include client_id
          window.parent.postMessage({
            type: 'SKF_REQUEST_TOKEN',
            client_id: CLIENT_ID
          }, '*');
          console.log('[wordpress-bridge] Token request sent to parent with client_id:', CLIENT_ID);

        } catch (err) {
          console.error('[wordpress-bridge] Error initializing bridge:', err);
          resolve(false);
        }
      });
    }

    function hideInitialLoader() {
      const loader = document.getElementById('initialLoader');
      if (loader) {
        loader.classList.add('fade-out');
        // Remove from DOM after animation completes
        setTimeout(() => loader.remove(), 300);
      }
    }

    (async ()=>{
      // First, try WordPress bridge auth if in iframe
      const bridgeAuth = await initWordPressBridge();

      // Then check session (bridge may have set it, or there may be an existing session)
      const ok = bridgeAuth || await ensureSession();

      // Hide loading overlay and show appropriate view
      hideInitialLoader();
      setAuthVisible(!ok);
      setChatVisible(ok);
      try { document.getElementById('voiceStartBtn').disabled = !ok; } catch {}

      // Initialize chat mode UI (show/hide tabs based on configuration)
      initChatModeUI();

      // Initialize Supertab if configured (non-blocking)
      if (SUPERTAB_CONFIG.enabled) {
        initSupertab().catch(err => {
          console.warn('[supertab] Background init failed:', err);
        });
      }

      // Auto-restore most recent conversation if authenticated
      if (ok) {
        await autoRestoreConversation();
      }
    })();

    // Clean up realtime subscription when page unloads to prevent orphaned connections
    window.addEventListener('beforeunload', () => {
      stopVoiceTranscriptStream();
    });

    // Pause/resume transcript streaming based on tab visibility
    document.addEventListener('visibilitychange', () => {
      if (document.hidden && transcriptRealtimeChannel) {
        try { console.log('[voice] Tab hidden, pausing transcript realtime channel'); } catch {}
        stopVoiceTranscriptStream();
      } else if (
        !document.hidden &&
        storedVoiceSession &&
        storedVoiceSession.conversationId &&
        !transcriptRealtimeChannel
      ) {
        try { console.log('[voice] Tab visible, resuming transcript realtime channel'); } catch {}
        startVoiceTranscriptStream(storedVoiceSession.conversationId, CLIENT_ID, storedVoiceSession.agentId);
      }
    });

    // ==========================================================================
    // Ambient Abilities - Session End Detection & Notifications
    // ==========================================================================

    let sessionMessageCount = 0;
    let sessionIdleTimeout = null;
    const SESSION_IDLE_THRESHOLD = 30000; // 30 seconds of inactivity to consider session ended

    function resetSessionIdleTimer() {
      if (sessionIdleTimeout) clearTimeout(sessionIdleTimeout);
      sessionIdleTimeout = setTimeout(notifySessionEnd, SESSION_IDLE_THRESHOLD);
    }

    async function notifySessionEnd() {
      if (!currentConversationId || sessionMessageCount < 1) return;

      const userId = getEffectiveUserId();
      if (!userId) return;

      try {
        console.log('[ambient] Notifying session end:', {
          conversationId: currentConversationId,
          messageCount: sessionMessageCount
        });

        const fd = new FormData();
        fd.append('client_id', CLIENT_ID);
        fd.append('user_id', userId);
        fd.append('conversation_id', currentConversationId);
        fd.append('message_count', sessionMessageCount.toString());
        fd.append('agent_slug', AGENT_SLUG);

        const bearer = manualToken || (session && session.access_token) || '';
        await fetch('/api/embed/session-end', {
          method: 'POST',
          body: fd,
          headers: { Authorization: `Bearer ${bearer}` }
        });

        // Reset for next session
        sessionMessageCount = 0;

        // Start polling for UserSense completion notification
        // UserSense typically takes 5-15 seconds to process
        startNotificationPolling();
      } catch (err) {
        console.warn('[ambient] Failed to notify session end:', err);
      }
    }

    // Poll for notifications after session-end to show real-time feedback
    let notificationPollInterval = null;
    let notificationPollCount = 0;
    const MAX_NOTIFICATION_POLLS = 12; // Poll for up to 60 seconds (12 * 5s)

    function startNotificationPolling() {
      // Clear any existing polling
      if (notificationPollInterval) {
        clearInterval(notificationPollInterval);
      }
      notificationPollCount = 0;

      console.log('[ambient] Starting notification polling after session end');

      // Poll every 5 seconds for UserSense completion
      notificationPollInterval = setInterval(async () => {
        notificationPollCount++;

        // Stop polling after max attempts
        if (notificationPollCount >= MAX_NOTIFICATION_POLLS) {
          console.log('[ambient] Stopping notification polling (max attempts reached)');
          clearInterval(notificationPollInterval);
          notificationPollInterval = null;
          return;
        }

        // Check for notifications
        const found = await checkAmbientNotifications();

        // Stop polling if we found and showed a notification
        if (found) {
          console.log('[ambient] Notification found, stopping polling');
          clearInterval(notificationPollInterval);
          notificationPollInterval = null;
        }
      }, 5000);
    }

    function stopNotificationPolling() {
      if (notificationPollInterval) {
        clearInterval(notificationPollInterval);
        notificationPollInterval = null;
      }
    }

    // Track message count for session
    function incrementSessionMessageCount() {
      sessionMessageCount++;
      resetSessionIdleTimer();
    }

    // Check for ambient notifications on load and periodically
    // Returns true if a notification was found and shown
    async function checkAmbientNotifications() {
      const userId = getEffectiveUserId();
      if (!userId) return false;

      try {
        const bearer = manualToken || (session && session.access_token) || '';
        const res = await fetch(
          `/api/embed/notifications?client_id=${CLIENT_ID}&user_id=${userId}`,
          { headers: { Authorization: `Bearer ${bearer}` } }
        );

        if (!res.ok) return false;

        const data = await res.json();
        if (data.notifications && data.notifications.length > 0) {
          const notification = data.notifications[0];
          showAmbientNotification(notification.message || 'User Understanding Expanded');

          // Mark as shown
          await fetch(`/api/embed/notifications/${notification.id}/shown`, {
            method: 'POST',
            headers: { Authorization: `Bearer ${bearer}` }
          });
          return true;
        }
        return false;
      } catch (err) {
        console.warn('[ambient] Failed to check notifications:', err);
        return false;
      }
    }

    function showAmbientNotification(message) {
      const el = document.getElementById('ambientNotification');
      if (!el) return;

      // Update message text
      const textEl = el.querySelector('.notification-text');
      if (textEl) textEl.textContent = message;

      // Show with animation
      el.classList.add('show');

      // Hide after 4 seconds
      setTimeout(() => {
        el.classList.remove('show');
      }, 4000);
    }

    // Notify session end when user leaves page - use sendBeacon for reliability
    function notifySessionEndSync() {
      if (!currentConversationId || sessionMessageCount < 1) return;

      const userId = getEffectiveUserId();
      if (!userId) return;

      console.log('[ambient] Sending session end beacon:', {
        conversationId: currentConversationId,
        messageCount: sessionMessageCount
      });

      // Use sendBeacon for reliable delivery during page unload
      const fd = new FormData();
      fd.append('client_id', CLIENT_ID);
      fd.append('user_id', userId);
      fd.append('conversation_id', currentConversationId);
      fd.append('message_count', sessionMessageCount.toString());
      fd.append('agent_slug', AGENT_SLUG);

      // sendBeacon is fire-and-forget but reliable during unload
      navigator.sendBeacon('/api/embed/session-end', fd);

      // Reset to prevent duplicate sends
      sessionMessageCount = 0;
    }

    window.addEventListener('beforeunload', notifySessionEndSync);

    // Also trigger on visibility change (tab switch, minimize)
    document.addEventListener('visibilitychange', () => {
      if (document.visibilityState === 'hidden' && sessionMessageCount >= 1) {
        notifySessionEndSync();
      }
    });

    // Also trigger on pagehide (more reliable on mobile)
    window.addEventListener('pagehide', notifySessionEndSync);

    // Check for notifications when page loads (after auth)
    setTimeout(() => {
      if (session || manualToken) {
        checkAmbientNotifications();
      }
    }, 2000);

    // Periodically check for notifications (every 60 seconds)
    setInterval(() => {
      if (session || manualToken) {
        checkAmbientNotifications();
      }
    }, 60000);
  </script>

  <!-- Disabled mode tooltip -->
  <div id="disabledModeTooltip" class="disabled-mode-tooltip hidden">
    <span id="disabledModeTooltipText">This mode is not available</span>
  </div>
</body>
</html>

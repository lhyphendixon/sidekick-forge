"""Shared helpers for applying Sidekick Forge schema patches to Supabase projects."""
from __future__ import annotations

from typing import Iterable, List, Tuple

import requests

from app.config import settings

# SQL statements maintained as canonical schema patches

# Minimal base schema for fresh tenant projects so later patches don't fail on
# missing tables. This is intentionally conservative and only creates tables
# that other patches expect; columns match the fields used by the app's
# document/transcript code paths.
BASE_SCHEMA_SQL = """
create extension if not exists "uuid-ossp";
create extension if not exists "pgcrypto";
create extension if not exists "vector";

create table if not exists public.conversations (
  id uuid primary key default gen_random_uuid(),
  session_id uuid,
  agent_id uuid,
  user_id uuid,
  metadata jsonb default '{}'::jsonb,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

create table if not exists public.conversation_transcripts (
  id uuid primary key default gen_random_uuid(),
  conversation_id uuid,
  session_id uuid,
  agent_id uuid,
  user_id uuid,
  role text,
  content text,
  transcript text,
  turn_id uuid,
  citations jsonb default '[]'::jsonb,
  metadata jsonb default '{}'::jsonb,
  source text,
  embeddings vector(1024),
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

create table if not exists public.agents (
  id uuid primary key default gen_random_uuid(),
  slug text not null unique,
  name text not null,
  description text,
  system_prompt text not null,
  agent_image text,
  voice_settings jsonb default '{}'::jsonb,
  webhooks jsonb default '{}'::jsonb,
  tools_config jsonb,
  enabled boolean default true,
  show_citations boolean default true,
  rag_results_limit int default 5,
  model text,
  context_retention_minutes int default 30,
  max_context_messages int default 50,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

-- NOTE: Existing tenant databases use bigint for documents.id (auto-increment).
-- New tenants will also use bigint for consistency across all projects.
create table if not exists public.documents (
  id bigint primary key generated by default as identity,
  user_id uuid,
  agent_id uuid,
  title text,
  filename text,
  file_name text,
  file_size bigint,
  file_type text,
  content text,
  status text default 'pending',
  upload_status text default 'pending',
  processing_status text default 'pending',
  document_type text default 'knowledge_base',
  chunk_count int default 0,
  word_count int,
  metadata jsonb default '{}'::jsonb,
  processing_metadata jsonb default '{}'::jsonb,
  embedding vector(1024),
  embedding_vec vector(1024),
  embeddings vector(1024),
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

create table if not exists public.document_chunks (
  id uuid primary key default gen_random_uuid(),
  document_id bigint references public.documents(id) on delete cascade,
  chunk_index int,
  content text,
  embeddings vector(1024),
  embeddings_vec vector(1024),
  chunk_metadata jsonb default '{}'::jsonb,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

create table if not exists public.agent_documents (
  id uuid primary key default gen_random_uuid(),
  agent_id uuid not null,
  document_id bigint not null references public.documents(id) on delete cascade,
  access_type text default 'read',
  enabled boolean default true,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now(),
  unique(agent_id, document_id)
);
""".strip()

# Align vector dimensions on existing columns in case they were created without
# a specified length in earlier runs.
VECTOR_DIMENSION_PATCH_SQL = """
do $$
begin
  if exists (select 1 from information_schema.columns where table_name = 'conversation_transcripts' and column_name = 'embeddings') then
    alter table public.conversation_transcripts
      alter column embeddings type vector(1024);
  end if;

  if exists (select 1 from information_schema.columns where table_name = 'documents' and column_name = 'embeddings') then
    alter table public.documents
      alter column embeddings type vector(1024);
  end if;

  alter table public.documents
    add column if not exists file_name text,
    add column if not exists file_size bigint,
    add column if not exists file_type text,
    add column if not exists document_type text default 'knowledge_base',
    add column if not exists processing_metadata jsonb default '{}'::jsonb;

  alter table public.documents
    add column if not exists embedding vector(1024),
    add column if not exists embedding_vec vector(1024);

  if exists (select 1 from information_schema.columns where table_name = 'documents' and column_name = 'embedding') then
    alter table public.documents
      alter column embedding type vector(1024);
  end if;

  if exists (select 1 from information_schema.columns where table_name = 'documents' and column_name = 'embedding_vec') then
    alter table public.documents
      alter column embedding_vec type vector(1024);
  end if;

  if exists (select 1 from information_schema.columns where table_name = 'document_chunks' and column_name = 'embeddings') then
    alter table public.document_chunks
      alter column embeddings type vector(1024);
  end if;

  if exists (select 1 from information_schema.columns where table_name = 'document_chunks' and column_name = 'embeddings_vec') then
    alter table public.document_chunks
      alter column embeddings_vec type vector(1024);
  end if;
end$$;
""".strip()

CONVERSATION_PATCH_SQL = """
alter table if exists public.conversation_transcripts
  add column if not exists role text,
  add column if not exists sequence int,
  add column if not exists user_message text,
  add column if not exists assistant_message text,
  add column if not exists citations jsonb default '[]'::jsonb,
  add column if not exists source text,
  add column if not exists turn_id uuid default gen_random_uuid();

update public.conversation_transcripts
  set turn_id = coalesce(turn_id, gen_random_uuid());
""".strip()

IVFFLAT_PATCH_SQL = """
create index if not exists documents_embeddings_ivfflat
  on public.documents using ivfflat (embeddings vector_cosine_ops) with (lists = 16);

create index if not exists documents_embedding_ivfflat
  on public.documents using ivfflat (embedding vector_cosine_ops) with (lists = 16);

create index if not exists documents_embedding_vec_ivfflat
  on public.documents using ivfflat (embedding_vec vector_cosine_ops) with (lists = 16);

create index if not exists document_chunks_embeddings_ivfflat
  on public.document_chunks using ivfflat (embeddings vector_cosine_ops) with (lists = 16);

create index if not exists document_chunks_embeddings_vec_ivfflat
  on public.document_chunks using ivfflat (embeddings_vec vector_cosine_ops) with (lists = 16);

create index if not exists conversation_transcripts_embeddings_ivfflat
  on public.conversation_transcripts using ivfflat (embeddings vector_cosine_ops) with (lists = 16);
""".strip()

# RAG RPCs and missing helper tables/columns to keep tenant projects consistent
RAG_PATCH_SQL = """
-- Ensure profiles table exists for user context lookups
create table if not exists public.profiles (
  id uuid primary key default gen_random_uuid(),
  user_id uuid,
  email text,
  full_name text,
  metadata jsonb default '{}'::jsonb,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);
create index if not exists profiles_user_id_idx on public.profiles(user_id);

-- Ensure conversations has channel column for transcript storage
alter table if exists public.conversations
  add column if not exists channel text;

-- match_documents with agent filtering (1024-dim embeddings)
-- NOTE: Returns id as text for schema-agnostic compatibility (works with bigint or uuid)
create or replace function public.match_documents(
  p_query_embedding vector,
  p_agent_slug text,
  p_match_threshold float8,
  p_match_count integer
)
returns table(
  id text,
  title text,
  content text,
  similarity float8
)
language plpgsql
as $$
begin
  return query
  select
    d.id::text as id,
    coalesce(d.title, 'Untitled')::text as title,
    d.content,
    1 - (d.embeddings <=> p_query_embedding) as similarity
  from public.documents d
  join public.agent_documents ad on ad.document_id = d.id
  join public.agents a on ad.agent_id = a.id
  where a.slug = p_agent_slug
    and d.embeddings is not null
    and coalesce(1 - (d.embeddings <=> p_query_embedding), 0) > p_match_threshold
  order by d.embeddings <=> p_query_embedding
  limit p_match_count;
end;
$$;

-- match_conversation_transcripts_secure for user-specific transcript search
create or replace function public.match_conversation_transcripts_secure(
  query_embeddings vector,
  agent_slug_param text,
  user_id_param uuid,
  match_count integer default 5
)
returns table(
  conversation_id uuid,
  user_message text,
  agent_response text,
  similarity float8,
  created_at timestamptz
)
language plpgsql
as $$
begin
  return query
  select
    u.conversation_id,
    u.content as user_message,
    a.content as agent_response,
    1 - (u.embeddings <=> query_embeddings) as similarity,
    u.created_at
  from public.conversation_transcripts u
  join public.conversation_transcripts a
    on a.conversation_id = u.conversation_id and a.role = 'assistant'
  join public.agents ag on u.agent_id = ag.id
  where u.role = 'user'
    and u.embeddings is not null
    and u.user_id = user_id_param
    and ag.slug = agent_slug_param
  order by u.embeddings <=> query_embeddings
  limit match_count;
end;
$$;

grant execute on function public.match_documents(vector, text, float8, integer) to anon, authenticated, service_role;
grant execute on function public.match_conversation_transcripts_secure(vector, text, uuid, integer) to anon, authenticated, service_role;

-- Ensure per-agent RAG result limits exist
alter table if exists public.agents
  add column if not exists rag_results_limit int default 5;
""".strip()

SQL_ENDPOINT_TEMPLATE = "https://api.supabase.com/v1/projects/{project_ref}/database/query"


class SchemaSyncError(RuntimeError):
    """Raised when schema sync fails for a Supabase project."""


def project_ref_from_url(url: str) -> str:
    """Extract the Supabase project ref from the provided URL."""
    host = url.split("https://")[-1]
    return host.split(".supabase.co")[0]


def execute_sql(project_ref: str, token: str, sql: str) -> Tuple[bool, str]:
    """Execute raw SQL against a Supabase project via Management API."""
    url = SQL_ENDPOINT_TEMPLATE.format(project_ref=project_ref)
    headers = {
        "Authorization": f"Bearer {token}",
        "apikey": token,
        "Content-Type": "application/json",
    }
    payload = {"query": sql}
    response = requests.post(url, headers=headers, json=payload, timeout=30)
    if response.status_code in (200, 201):
        return True, ""
    try:
        detail = response.json().get("msg") or response.text
    except Exception:
        detail = response.text
    return False, detail


def apply_schema(project_ref: str, token: str, include_indexes: bool = True) -> List[Tuple[str, bool, str]]:
    """Apply canonical schema patches to the given Supabase project.

    Returns a list of (step, success, detail) tuples for logging/telemetry.
    """
    results: List[Tuple[str, bool, str]] = []

    ok_base, detail_base = execute_sql(project_ref, token, BASE_SCHEMA_SQL)
    results.append(("base_schema", ok_base, detail_base))

    ok_vectors, detail_vectors = execute_sql(project_ref, token, VECTOR_DIMENSION_PATCH_SQL)
    results.append(("vector_dimensions", ok_vectors, detail_vectors))

    ok, detail = execute_sql(project_ref, token, CONVERSATION_PATCH_SQL)
    results.append(("conversation_patch", ok, detail))

    ok_rag, detail_rag = execute_sql(project_ref, token, RAG_PATCH_SQL)
    results.append(("rag_patch", ok_rag, detail_rag))

    if include_indexes:
        ok_indexes, detail_indexes = execute_sql(project_ref, token, IVFFLAT_PATCH_SQL)
        results.append(("ivfflat_indexes", ok_indexes, detail_indexes))

    return results


def fetch_platform_clients(token: str) -> List[dict]:
    """Fetch clients from the platform database using the Management API."""
    project_ref = project_ref_from_url(settings.supabase_url)
    url = SQL_ENDPOINT_TEMPLATE.format(project_ref=project_ref)
    headers = {
        "Authorization": f"Bearer {token}",
        "apikey": token,
        "Content-Type": "application/json",
    }
    payload = {
        "query": (
            "select id, name, supabase_url, supabase_service_role_key, provisioning_status "
            "from clients"
        )
    }
    response = requests.post(url, headers=headers, json=payload, timeout=30)
    response.raise_for_status()
    data = response.json()
    if isinstance(data, dict):
        return data.get("result") or data.get("data") or []
    if isinstance(data, list):
        return data
    return []


__all__ = [
    "SchemaSyncError",
    "apply_schema",
    "project_ref_from_url",
    "execute_sql",
    "fetch_platform_clients",
]
